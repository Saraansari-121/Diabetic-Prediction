{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis with categorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yup-06JsuzSz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvZAsXWEuYZt"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Diabetes_raw_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBDl_HZ4Bk3r",
    "outputId": "19a80a51-2c30-41be-8f17-94d46d9cd9d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Patient number', ' Cholesterol (mg/dl)', 'Glucose (mg/dl)',\n",
       "       'HDL Chol (mg/dl)', 'TChol/HDL ratio', 'Age', 'Gender',\n",
       "       'Height /stature (cm)', 'weight1 (Kg)', 'weight2(Kg)', 'BMI (Kg/m^2)',\n",
       "       'Systolic BP', 'Diastolic BP', 'waist (cm)', 'hip (cm)', 'Diabetes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEMz94ERBxLR",
    "outputId": "7369c303-db9e-47eb-de24-132327478864"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      193.0\n",
       "1      146.0\n",
       "2      217.0\n",
       "3      226.0\n",
       "4      164.0\n",
       "       ...  \n",
       "495    155.0\n",
       "496    179.0\n",
       "497    283.0\n",
       "498    228.0\n",
       "499    220.0\n",
       "Name:  Cholesterol (mg/dl), Length: 500, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[' Cholesterol (mg/dl)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting null and zero cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3gop4FMxg77",
    "outputId": "39bab476-3855-4c49-cf80-3c99fadead60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of zeros in column  Patient number  is :  0\n",
      "Count of zeros in column   Cholesterol (mg/dl)  is :  0\n",
      "Count of zeros in column  Glucose (mg/dl)  is :  0\n",
      "Count of zeros in column  HDL Chol (mg/dl)  is :  0\n",
      "Count of zeros in column  TChol/HDL ratio  is :  0\n",
      "Count of zeros in column  Age  is :  0\n",
      "Count of zeros in column  Gender  is :  0\n",
      "Count of zeros in column  Height /stature (cm)  is :  2\n",
      "Count of zeros in column  weight1 (Kg)  is :  2\n",
      "Count of zeros in column  weight2(Kg)  is :  2\n",
      "Count of zeros in column  BMI (Kg/m^2)  is :  4\n",
      "Count of zeros in column  Systolic BP  is :  0\n",
      "Count of zeros in column  Diastolic BP  is :  0\n",
      "Count of zeros in column  waist (cm)  is :  2\n",
      "Count of zeros in column  hip (cm)  is :  0\n",
      "Count of zeros in column  Diabetes  is :  0\n"
     ]
    }
   ],
   "source": [
    "for column_name in dataset.columns:\n",
    "    column = dataset[column_name]\n",
    "    # Get the count of Zeros in column \n",
    "    count = (column == 0).sum()\n",
    "    print('Count of zeros in column ', column_name, ' is : ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vF7HyGiP1CYK",
    "outputId": "7662da2c-5c43-40b8-b531-2261c1dda619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of null in column  Patient number  is :  0\n",
      "Count of null in column   Cholesterol (mg/dl)  is :  1\n",
      "Count of null in column  Glucose (mg/dl)  is :  1\n",
      "Count of null in column  HDL Chol (mg/dl)  is :  1\n",
      "Count of null in column  TChol/HDL ratio  is :  1\n",
      "Count of null in column  Age  is :  0\n",
      "Count of null in column  Gender  is :  5\n",
      "Count of null in column  Height /stature (cm)  is :  0\n",
      "Count of null in column  weight1 (Kg)  is :  0\n",
      "Count of null in column  weight2(Kg)  is :  0\n",
      "Count of null in column  BMI (Kg/m^2)  is :  0\n",
      "Count of null in column  Systolic BP  is :  2\n",
      "Count of null in column  Diastolic BP  is :  2\n",
      "Count of null in column  waist (cm)  is :  0\n",
      "Count of null in column  hip (cm)  is :  0\n",
      "Count of null in column  Diabetes  is :  7\n"
     ]
    }
   ],
   "source": [
    "for column_name in dataset.columns:\n",
    "    column = dataset[column_name]\n",
    "    # Get the count of null in column \n",
    "    count = (column).isnull().sum()\n",
    "    print('Count of null in column ', column_name, ' is : ', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlw-hTY5x7I1"
   },
   "source": [
    "## gender label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDD-1FJQGpGL"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dataset['Gender'] =  le.fit_transform(dataset['Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGdONv1Ox2sE"
   },
   "source": [
    "# splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF-IT_BZx57F"
   },
   "outputs": [],
   "source": [
    "x = dataset.iloc[:, 1:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIjuPa1bMsyS",
    "outputId": "112e251c-ab57-4ee6-978f-a8eb4e330076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coulmn number of x\n",
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "3jW2j7r30hwz",
    "outputId": "c415792d-5217-417a-867a-da88a6df365b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f4578d8d-d0e4-4f94-99ad-b1729b990c5e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient number</th>\n",
       "      <th>Cholesterol (mg/dl)</th>\n",
       "      <th>Glucose (mg/dl)</th>\n",
       "      <th>HDL Chol (mg/dl)</th>\n",
       "      <th>TChol/HDL ratio</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height /stature (cm)</th>\n",
       "      <th>weight1 (Kg)</th>\n",
       "      <th>weight2(Kg)</th>\n",
       "      <th>BMI (Kg/m^2)</th>\n",
       "      <th>Systolic BP</th>\n",
       "      <th>Diastolic BP</th>\n",
       "      <th>waist (cm)</th>\n",
       "      <th>hip (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>208.248497</td>\n",
       "      <td>128.074148</td>\n",
       "      <td>50.336673</td>\n",
       "      <td>4.467936</td>\n",
       "      <td>46.488000</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>166.934600</td>\n",
       "      <td>82.46394</td>\n",
       "      <td>80.028000</td>\n",
       "      <td>29.694020</td>\n",
       "      <td>139.672691</td>\n",
       "      <td>82.672691</td>\n",
       "      <td>95.82800</td>\n",
       "      <td>109.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.481833</td>\n",
       "      <td>50.676848</td>\n",
       "      <td>414.073059</td>\n",
       "      <td>17.141593</td>\n",
       "      <td>1.652604</td>\n",
       "      <td>17.629278</td>\n",
       "      <td>0.517106</td>\n",
       "      <td>14.551164</td>\n",
       "      <td>46.39359</td>\n",
       "      <td>18.888111</td>\n",
       "      <td>18.010717</td>\n",
       "      <td>56.976674</td>\n",
       "      <td>13.599003</td>\n",
       "      <td>15.86429</td>\n",
       "      <td>14.299882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>76.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>125.750000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>68.10000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>22.980000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>83.80000</td>\n",
       "      <td>99.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>167.600000</td>\n",
       "      <td>79.00000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>28.130000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>94.00000</td>\n",
       "      <td>106.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>375.250000</td>\n",
       "      <td>229.500000</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>175.300000</td>\n",
       "      <td>90.80000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>33.830000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>104.10000</td>\n",
       "      <td>116.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>9091.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>996.98000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>377.280000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>142.20000</td>\n",
       "      <td>162.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4578d8d-d0e4-4f94-99ad-b1729b990c5e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f4578d8d-d0e4-4f94-99ad-b1729b990c5e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f4578d8d-d0e4-4f94-99ad-b1729b990c5e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       Patient number   Cholesterol (mg/dl)  Glucose (mg/dl)  \\\n",
       "count      500.000000            499.000000       499.000000   \n",
       "mean       250.500000            208.248497       128.074148   \n",
       "std        144.481833             50.676848       414.073059   \n",
       "min          1.000000              2.000000        48.000000   \n",
       "25%        125.750000            178.000000        81.000000   \n",
       "50%        250.500000            204.000000        89.000000   \n",
       "75%        375.250000            229.500000       106.500000   \n",
       "max        500.000000            500.000000      9091.000000   \n",
       "\n",
       "       HDL Chol (mg/dl)  TChol/HDL ratio         Age      Gender  \\\n",
       "count        499.000000       499.000000  500.000000  500.000000   \n",
       "mean          50.336673         4.467936   46.488000    0.444000   \n",
       "std           17.141593         1.652604   17.629278    0.517106   \n",
       "min            1.000000         1.500000   19.000000    0.000000   \n",
       "25%           39.000000         3.200000   31.000000    0.000000   \n",
       "50%           46.000000         4.200000   43.000000    0.000000   \n",
       "75%           59.000000         5.300000   61.000000    1.000000   \n",
       "max          120.000000        19.300000   92.000000    2.000000   \n",
       "\n",
       "       Height /stature (cm)  weight1 (Kg)  weight2(Kg)  BMI (Kg/m^2)  \\\n",
       "count            500.000000     500.00000   500.000000    500.000000   \n",
       "mean             166.934600      82.46394    80.028000     29.694020   \n",
       "std               14.551164      46.39359    18.888111     18.010717   \n",
       "min                0.000000       0.00000     0.000000      0.000000   \n",
       "25%              160.000000      68.10000    68.000000     22.980000   \n",
       "50%              167.600000      79.00000    78.000000     28.130000   \n",
       "75%              175.300000      90.80000    90.000000     33.830000   \n",
       "max              193.000000     996.98000   148.000000    377.280000   \n",
       "\n",
       "       Systolic BP  Diastolic BP  waist (cm)    hip (cm)  \n",
       "count   498.000000    498.000000   500.00000  500.000000  \n",
       "mean    139.672691     82.672691    95.82800  109.113200  \n",
       "std      56.976674     13.599003    15.86429   14.299882  \n",
       "min      90.000000     48.000000     0.00000   76.200000  \n",
       "25%     120.000000     74.000000    83.80000   99.100000  \n",
       "50%     136.000000     82.000000    94.00000  106.700000  \n",
       "75%     148.000000     90.000000   104.10000  116.800000  \n",
       "max    1140.000000    124.000000   142.20000  162.600000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKJtLT0SMuuB"
   },
   "source": [
    "# Detecting outlier using Z score\n",
    "Outliers have been detected by methods such as 'Z Score' and 'Interquartile range (IQR)'. The ‘Z Score’ method is used for data with normal distribution, and in this task, a distance of 2 standard deviations from the mean is considered to find outlier data. It should be noted that in this section, not all trends can be considered outliers. For example, in the column related to ‘cholesterol’, only 2 is an outlier and other data are medically meaningful. So outlier data were selected with the opinion of the medical team.\n",
    "\n",
    "z = (X — μ) / σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBiWOBL3Jvjm",
    "outputId": "50cc2300-e477-4b8e-a57f-89a23d2b92c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Patient number\n",
      "outliers : [] \n",
      "--------------------- Cholesterol (mg/dl)\n",
      "outliers : [337.0, 347.0, 322.0, 443.0, 404.0, 318.0, 342.0, 78.0, 78.0, 500.0, 450.0, 479.0, 413.0, 337.0, 2.0] \n",
      "---------------------Glucose (mg/dl)\n",
      "outliers : [1975.0, 9091.0] \n",
      "---------------------HDL Chol (mg/dl)\n",
      "outliers : [117.0, 92.0, 85.0, 94.0, 91.0, 92.0, 92.0, 86.0, 88.0, 87.0, 87.0, 90.0, 14.0, 90.0, 108.0, 120.0, 92.0, 12.0, 110.0, 100.0, 87.0, 118.0, 114.0, 12.0, 110.0, 100.0, 1.0, 94.0, 91.0, 85.0] \n",
      "---------------------TChol/HDL ratio\n",
      "outliers : [8.3, 9.4, 7.9, 7.9, 8.9, 19.3, 10.6, 12.2, 8.7, 8.0, 7.8, 8.9, 7.8] \n",
      "---------------------Age\n",
      "outliers : [82, 82, 83, 84, 89, 91, 92] \n",
      "---------------------Gender\n",
      "outliers : [2, 2, 2, 2, 2] \n",
      "---------------------Height /stature (cm)\n",
      "outliers : [132.1, 0.0, 0.0] \n",
      "---------------------weight1 (Kg)\n",
      "outliers : [322.34, 996.98] \n",
      "---------------------weight2(Kg)\n",
      "outliers : [124, 126, 118, 131, 140, 126, 119, 131, 129, 120, 123, 148, 118, 121, 119, 145, 128, 132, 0, 140, 126, 0, 118] \n",
      "---------------------BMI (Kg/m^2)\n",
      "outliers : [83.28, 104.94, 377.28] \n",
      "---------------------Systolic BP\n",
      "outliers : [730.0, 1140.0] \n",
      "---------------------Diastolic BP\n",
      "outliers : [110.0, 48.0, 52.0, 124.0, 112.0, 122.0, 112.0, 110.0, 115.0, 110.0, 114.0, 120.0, 110.0, 110.0, 110.0, 120.0, 110.0, 50.0, 110.0, 110.0, 118.0, 50.0, 118.0, 115.0, 53.0, 50.0, 118.0, 115.0, 53.0, 124.0, 112.0, 48.0] \n",
      "---------------------waist (cm)\n",
      "outliers : [134.6, 132.1, 129.5, 134.6, 129.5, 142.2, 132.1, 139.7, 129.5, 0.0, 129.5, 129.5, 134.6, 132.1, 0.0] \n",
      "---------------------hip (cm)\n",
      "outliers : [147.3, 147.3, 139.7, 142.2, 147.3, 147.3, 144.8, 147.3, 139.7, 152.4, 76.2, 157.5, 162.6, 149.9, 144.8, 157.5, 139.7, 142.2, 147.3, 147.3, 147.3] \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = 0\n",
    "for column_name in dataset.columns:\n",
    "  print(\"---------------------\"+column_name)\n",
    "  outliers=[]\n",
    "  def detect_outliers(data):\n",
    "      \n",
    "      threshold=2\n",
    "      mean = np.mean(data)\n",
    "      std =np.std(data)\n",
    "      \n",
    "      \n",
    "      for i in data:\n",
    "          z_score= (i - mean)/std \n",
    "          if np.abs(z_score) > threshold:\n",
    "              outliers.append(i)\n",
    "      return outliers\n",
    "\n",
    "  outlier_pt =detect_outliers(dataset[column_name])\n",
    "  print(f\"outliers : {outlier_pt} \")\n",
    "\n",
    "  index += 1\n",
    "  if index == 15:\n",
    "      break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz0XsMXnMeAD"
   },
   "source": [
    "# Detect outliers using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gm4p-LWIYVon",
    "outputId": "b59f1419-1b2d-4d7d-c0e1-a012f019a3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Patient number\n",
      "Q1, Q3 are : (125.75, 375.25)\n",
      "IQR is : 249.5\n",
      "lower_limit, upper_limit are : (-248.5, 749.5)\n",
      "[]\n",
      "--------------------- Cholesterol (mg/dl)\n",
      "Q1, Q3 are : (178.0, 229.5)\n",
      "IQR is : 51.5\n",
      "lower_limit, upper_limit are : (100.75, 306.75)\n",
      "[337.0, 347.0, 322.0, 307.0, 443.0, 404.0, 318.0, 342.0, 78.0, 78.0, 500.0, 450.0, 479.0, 413.0, 337.0, 2.0]\n",
      "---------------------Glucose (mg/dl)\n",
      "Q1, Q3 are : (81.0, 106.5)\n",
      "IQR is : 25.5\n",
      "lower_limit, upper_limit are : (42.75, 144.75)\n",
      "[269.0, 155.0, 153.0, 197.0, 155.0, 299.0, 228.0, 187.0, 155.0, 225.0, 196.0, 233.0, 173.0, 255.0, 385.0, 182.0, 185.0, 297.0, 369.0, 193.0, 236.0, 173.0, 206.0, 176.0, 239.0, 248.0, 267.0, 270.0, 270.0, 223.0, 225.0, 330.0, 206.0, 145.0, 251.0, 177.0, 197.0, 172.0, 341.0, 174.0, 173.0, 371.0, 206.0, 203.0, 262.0, 342.0, 171.0, 235.0, 161.0, 279.0, 184.0, 173.0, 371.0, 206.0, 203.0, 262.0, 342.0, 171.0, 235.0, 269.0, 155.0, 1975.0, 9091.0]\n",
      "---------------------HDL Chol (mg/dl)\n",
      "Q1, Q3 are : (39.0, 59.0)\n",
      "IQR is : 20.0\n",
      "lower_limit, upper_limit are : (9.0, 89.0)\n",
      "[117.0, 92.0, 94.0, 91.0, 92.0, 92.0, 90.0, 90.0, 108.0, 120.0, 92.0, 110.0, 100.0, 118.0, 114.0, 110.0, 100.0, 1.0, 94.0, 91.0]\n",
      "---------------------TChol/HDL ratio\n",
      "Q1, Q3 are : (3.2, 5.3)\n",
      "IQR is : 2.0999999999999996\n",
      "lower_limit, upper_limit are : (0.05000000000000071, 8.45)\n",
      "[9.4, 8.9, 19.3, 10.6, 12.2, 8.7, 8.9]\n",
      "---------------------Age\n",
      "Q1, Q3 are : (31.0, 61.0)\n",
      "IQR is : 30.0\n",
      "lower_limit, upper_limit are : (-14.0, 106.0)\n",
      "[]\n",
      "---------------------Gender\n",
      "Q1, Q3 are : (0.0, 1.0)\n",
      "IQR is : 1.0\n",
      "lower_limit, upper_limit are : (-1.5, 2.5)\n",
      "[]\n",
      "---------------------Height /stature (cm)\n",
      "Q1, Q3 are : (160.0, 175.3)\n",
      "IQR is : 15.300000000000011\n",
      "lower_limit, upper_limit are : (137.04999999999998, 198.25000000000003)\n",
      "[132.1, 0.0, 0.0]\n",
      "---------------------weight1 (Kg)\n",
      "Q1, Q3 are : (68.1, 90.8)\n",
      "IQR is : 22.700000000000003\n",
      "lower_limit, upper_limit are : (34.04999999999999, 124.85)\n",
      "[130.75, 147.55, 125.76, 145.28, 139.83, 128.03, 131.66, 125.76, 129.39, 131.21, 0.0, 131.21, 322.34, 996.98, 0.0, 130.75]\n",
      "---------------------weight2(Kg)\n",
      "Q1, Q3 are : (68.0, 90.0)\n",
      "IQR is : 22.0\n",
      "lower_limit, upper_limit are : (35.0, 123.0)\n",
      "[124, 126, 131, 140, 126, 131, 129, 148, 145, 128, 132, 0, 140, 126, 0]\n",
      "---------------------BMI (Kg/m^2)\n",
      "Q1, Q3 are : (22.98, 33.83)\n",
      "IQR is : 10.849999999999998\n",
      "lower_limit, upper_limit are : (6.705000000000002, 50.105)\n",
      "[61.46, 54.15, 83.28, 50.13, 50.25, 0.0, 50.25, 104.94, 0.0, 0.0, 377.28, 0.0]\n",
      "---------------------Systolic BP\n",
      "Q1, Q3 are : (120.0, 148.0)\n",
      "IQR is : 28.0\n",
      "lower_limit, upper_limit are : (78.0, 190.0)\n",
      "[212.0, 230.0, 220.0, 200.0, 199.0, 250.0, 218.0, 199.0, 250.0, 730.0, 1140.0]\n",
      "---------------------Diastolic BP\n",
      "Q1, Q3 are : (74.0, 90.0)\n",
      "IQR is : 16.0\n",
      "lower_limit, upper_limit are : (50.0, 114.0)\n",
      "[48.0, 124.0, 122.0, 115.0, 120.0, 120.0, 118.0, 118.0, 115.0, 118.0, 115.0, 124.0, 48.0]\n",
      "---------------------waist (cm)\n",
      "Q1, Q3 are : (83.8, 104.1)\n",
      "IQR is : 20.299999999999997\n",
      "lower_limit, upper_limit are : (53.35, 134.54999999999998)\n",
      "[134.6, 134.6, 142.2, 139.7, 0.0, 134.6, 0.0]\n",
      "---------------------hip (cm)\n",
      "Q1, Q3 are : (99.1, 116.8)\n",
      "IQR is : 17.700000000000003\n",
      "lower_limit, upper_limit are : (72.54999999999998, 143.35)\n",
      "[147.3, 147.3, 147.3, 147.3, 144.8, 147.3, 152.4, 157.5, 162.6, 149.9, 144.8, 157.5, 147.3, 147.3, 147.3]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for column_name in dataset.columns:\n",
    "  print(\"---------------------\"+column_name)\n",
    "\n",
    "  Q1 = dataset[column_name].quantile(0.25)\n",
    "  Q3 = dataset[column_name].quantile(0.75)\n",
    "  print(f'Q1, Q3 are : {Q1, Q3}')\n",
    "\n",
    "  IQR = Q3 - Q1\n",
    "  print(f'IQR is : {IQR}')\n",
    "\n",
    "  lower_limit = Q1 - 1.5*IQR\n",
    "  upper_limit = Q3 + 1.5*IQR\n",
    "  print(f'lower_limit, upper_limit are : {lower_limit, upper_limit}')\n",
    "\n",
    "  d = dataset[column_name][(dataset[column_name]<lower_limit)|(dataset[column_name]>upper_limit)]\n",
    "  print(d.values.tolist())\n",
    "\n",
    "  index += 1\n",
    "  if index == 15:\n",
    "      break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing and Categorizing columns  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dXBBNvUQufR"
   },
   "source": [
    "# imputing column 1 ( Cholesterol (mg/dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6zWYZNCRAby"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 1\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "●A total cholesterol level of less than 200 mg/dL (5.17 mmol/L) is normal. == [0]\n",
    "\n",
    "●A total cholesterol level of 200 to 239 mg/dL (5.17 to 6.18 mmol/L) is borderline high.== [1]\n",
    "\n",
    "●A total cholesterol level of 240 mg/dL (6.21 mmol/L) or greater is high. == [2]\n",
    "\n",
    "\n",
    "outlires : \n",
    "[2.0]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2wm2LNoeiPj"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxTQ97BVck5U"
   },
   "outputs": [],
   "source": [
    "c0 = np.reshape(x[:,0], (-1, 1))\n",
    "c0[c0 == 2] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0vfV7GOej4c"
   },
   "source": [
    "##imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqCOA38bvAxz"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "c0 = imputer.fit_transform(c0)\n",
    "x[:,0] = c0[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7DW7LpMPHsE",
    "outputId": "b1d30d70-dbd9-4d55-abcd-40b6cfc018ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([193.       , 146.       , 217.       , 226.       , 164.       ,\n",
       "       170.       , 149.       , 164.       , 230.       , 179.       ,\n",
       "       174.       , 193.       , 132.       , 203.       , 135.       ,\n",
       "       187.       , 244.       , 193.       , 165.       , 172.       ,\n",
       "       217.       , 223.       , 136.       , 175.       , 230.       ,\n",
       "       147.       , 229.       , 179.       , 185.       , 164.       ,\n",
       "       228.       , 199.       , 134.       , 169.       , 227.       ,\n",
       "       149.       , 155.       , 179.       , 283.       , 228.       ,\n",
       "       220.       , 170.       , 201.       , 238.       , 190.       ,\n",
       "       203.       , 226.       , 204.       , 166.       , 241.       ,\n",
       "       164.       , 214.       , 151.       , 184.       , 168.       ,\n",
       "       146.       , 189.       , 132.       , 179.       , 135.       ,\n",
       "       163.       , 204.       , 165.       , 181.       , 194.       ,\n",
       "       158.       , 160.       , 181.       , 144.       , 182.       ,\n",
       "       145.       , 195.       , 207.       , 192.       , 183.       ,\n",
       "       176.       , 163.       , 188.       , 209.       , 179.       ,\n",
       "       293.       , 305.       , 191.       , 155.       , 179.       ,\n",
       "       176.       , 244.       , 213.       , 217.       , 151.       ,\n",
       "       168.       , 231.       , 262.       , 179.       , 300.       ,\n",
       "       248.       , 217.       , 224.       , 171.       , 174.       ,\n",
       "       194.       , 150.       , 337.       , 239.       , 218.       ,\n",
       "       122.       , 225.       , 160.       , 191.       , 199.       ,\n",
       "       197.       , 179.       , 186.       , 178.       , 224.       ,\n",
       "       194.       , 347.       , 245.       , 227.       , 192.       ,\n",
       "       215.       , 214.       , 243.       , 156.       , 179.       ,\n",
       "       209.       , 232.       , 212.       , 199.       , 145.       ,\n",
       "       206.       , 147.       , 302.       , 138.       , 215.       ,\n",
       "       159.       , 268.       , 251.       , 216.       , 203.       ,\n",
       "       181.       , 239.       , 200.       , 211.       , 152.       ,\n",
       "       173.       , 215.       , 219.       , 180.       , 214.       ,\n",
       "       171.       , 183.       , 184.       , 180.       , 191.       ,\n",
       "       218.       , 169.       , 267.       , 234.       , 206.       ,\n",
       "       184.       , 178.       , 179.       , 225.       , 189.       ,\n",
       "       199.       , 241.       , 170.       , 269.       , 269.       ,\n",
       "       270.       , 172.       , 193.       , 199.       , 177.       ,\n",
       "       191.       , 208.       , 188.       , 243.       , 173.       ,\n",
       "       162.       , 322.       , 254.       , 160.       , 192.       ,\n",
       "       197.       , 237.       , 190.       , 190.       , 202.       ,\n",
       "       244.       , 168.       , 260.       , 214.       , 207.       ,\n",
       "       203.       , 189.       , 216.       , 233.       , 177.       ,\n",
       "       191.       , 142.       , 219.       , 190.       , 203.       ,\n",
       "       207.       , 242.       , 183.       , 234.       , 118.       ,\n",
       "       266.       , 223.       , 245.       , 173.       , 172.       ,\n",
       "       190.       , 134.       , 268.       , 209.       , 201.       ,\n",
       "       204.       , 307.       , 189.       , 160.       , 237.       ,\n",
       "       158.       , 255.       , 196.       , 185.       , 293.       ,\n",
       "       188.       , 174.       , 158.       , 181.       , 140.       ,\n",
       "       192.       , 284.       , 222.       , 249.       , 212.       ,\n",
       "       215.       , 218.       , 443.       , 218.       , 171.       ,\n",
       "       255.       , 182.       , 206.       , 261.       , 204.       ,\n",
       "       196.       , 219.       , 273.       , 225.       , 185.       ,\n",
       "       242.       , 296.       , 228.       , 194.       , 216.       ,\n",
       "       240.       , 148.       , 271.       , 204.       , 174.       ,\n",
       "       157.       , 263.       , 160.       , 179.       , 208.       ,\n",
       "       129.       , 219.       , 404.       , 138.       , 173.       ,\n",
       "       209.       , 228.       , 227.       , 201.       , 251.       ,\n",
       "       211.       , 115.       , 204.       , 215.       , 221.       ,\n",
       "       220.       , 193.       , 195.       , 219.       , 289.       ,\n",
       "       198.       , 192.       , 242.       , 235.       , 277.       ,\n",
       "       162.       , 318.       , 279.       , 128.       , 203.       ,\n",
       "       143.       , 300.       , 206.       , 182.       , 198.       ,\n",
       "       211.       , 265.       , 204.       , 169.       , 236.       ,\n",
       "       235.       , 196.       , 180.       , 194.       , 194.       ,\n",
       "       212.       , 293.       , 194.       , 277.       , 157.       ,\n",
       "       283.       , 215.       , 342.       , 202.       , 255.       ,\n",
       "       181.       , 249.       , 249.       , 219.       , 229.       ,\n",
       "       212.       , 170.       , 159.       , 224.       , 263.       ,\n",
       "       184.       , 281.       , 221.       , 188.       , 246.       ,\n",
       "       204.       ,  78.       , 206.       , 174.       , 254.       ,\n",
       "       198.       , 143.       , 207.       , 236.       , 260.       ,\n",
       "       242.       , 186.       , 182.       , 289.       , 231.       ,\n",
       "       199.       , 228.       , 213.       , 204.       , 205.       ,\n",
       "       213.       , 207.       , 235.       , 237.       , 306.       ,\n",
       "       223.       , 296.       , 205.       , 254.       , 159.       ,\n",
       "       196.       , 173.       , 219.       , 209.       , 215.       ,\n",
       "       210.       , 224.       , 195.       , 235.       , 292.       ,\n",
       "       157.       , 252.       , 271.       , 240.       , 255.       ,\n",
       "       227.       , 226.       , 301.       , 232.       , 165.       ,\n",
       "       204.       ,  78.       , 500.       , 174.       , 254.       ,\n",
       "       198.       , 143.       , 207.       , 236.       , 260.       ,\n",
       "       242.       , 186.       , 182.       , 289.       , 231.       ,\n",
       "       199.       , 228.       , 213.       , 450.       , 205.       ,\n",
       "       213.       , 207.       , 235.       , 237.       , 306.       ,\n",
       "       223.       , 296.       , 208.6626506, 254.       , 159.       ,\n",
       "       196.       , 173.       , 219.       , 209.       , 215.       ,\n",
       "       210.       , 224.       , 195.       , 235.       , 292.       ,\n",
       "       157.       , 204.       , 165.       , 181.       , 194.       ,\n",
       "       158.       , 160.       , 181.       , 144.       , 182.       ,\n",
       "       145.       , 195.       , 207.       , 192.       , 183.       ,\n",
       "       176.       , 163.       , 188.       , 209.       , 479.       ,\n",
       "       293.       , 305.       , 191.       , 155.       , 179.       ,\n",
       "       176.       , 244.       , 413.       , 217.       , 151.       ,\n",
       "       168.       , 231.       , 262.       , 179.       , 300.       ,\n",
       "       248.       , 217.       , 224.       , 171.       , 174.       ,\n",
       "       194.       , 150.       , 337.       , 239.       , 218.       ,\n",
       "       208.6626506, 225.       , 165.       , 172.       , 217.       ,\n",
       "       223.       , 136.       , 175.       , 230.       , 147.       ,\n",
       "       229.       , 179.       , 185.       , 164.       , 228.       ,\n",
       "       199.       , 134.       , 169.       , 227.       , 149.       ,\n",
       "       155.       , 179.       , 283.       , 228.       , 220.       ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DThgsjVRe9ti"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW2uGAWRfPrb"
   },
   "outputs": [],
   "source": [
    "c0[c0 < 200 ] = 0\n",
    "c0[np.logical_and(c0 < 240, c0 >=200)] = 1     # 200<x<240\n",
    "c0[ c0 >= 240 ] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZUCRAKTgvHO"
   },
   "outputs": [],
   "source": [
    "x[:,0] = c0[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0948dYkPD2N",
    "outputId": "f17e9844-b218-49d6-b0e6-93506b2a0659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 2.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 2., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 2., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 2., 2., 0., 0., 0.,\n",
       "       0., 2., 1., 1., 0., 0., 1., 2., 0., 2., 2., 1., 1., 0., 0., 0., 0.,\n",
       "       2., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 2., 2., 1.,\n",
       "       0., 1., 1., 2., 0., 0., 1., 1., 1., 0., 0., 1., 0., 2., 0., 1., 0.,\n",
       "       2., 2., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 2., 1., 1., 0., 0., 0., 1., 0., 0., 2., 0., 2., 2.,\n",
       "       2., 0., 0., 0., 0., 0., 1., 0., 2., 0., 0., 2., 2., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 2., 0., 2., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 2., 0., 1., 0., 2., 1., 2., 0., 0., 0., 0., 2., 1., 1., 1.,\n",
       "       2., 0., 0., 1., 0., 2., 0., 0., 2., 0., 0., 0., 0., 0., 0., 2., 1.,\n",
       "       2., 1., 1., 1., 2., 1., 0., 2., 0., 1., 2., 1., 0., 1., 2., 1., 0.,\n",
       "       2., 2., 1., 0., 1., 2., 0., 2., 1., 0., 0., 2., 0., 0., 1., 0., 1.,\n",
       "       2., 0., 0., 1., 1., 1., 1., 2., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       2., 0., 0., 2., 1., 2., 0., 2., 2., 0., 1., 0., 2., 1., 0., 0., 1.,\n",
       "       2., 1., 0., 1., 1., 0., 0., 0., 0., 1., 2., 0., 2., 0., 2., 1., 2.,\n",
       "       1., 2., 0., 2., 2., 1., 1., 1., 0., 0., 1., 2., 0., 2., 1., 0., 2.,\n",
       "       1., 0., 1., 0., 2., 0., 0., 1., 1., 2., 2., 0., 0., 2., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1., 2., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 2., 0., 2., 2., 2., 2., 1., 1., 2., 1., 0., 1.,\n",
       "       0., 2., 0., 2., 0., 0., 1., 1., 2., 2., 0., 0., 2., 1., 0., 1., 1.,\n",
       "       2., 1., 1., 1., 1., 1., 2., 1., 2., 1., 2., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 2., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 2., 2., 2., 0., 0., 0., 0., 2., 2., 1.,\n",
       "       0., 0., 1., 2., 0., 2., 2., 1., 1., 0., 0., 0., 0., 2., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 2., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5wgzehimPmN"
   },
   "source": [
    "# imputing column 2 (Glucose (mg/dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4JjR0h1mPmO"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 1\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "**Fasting Blood Sugar Test**\n",
    "\n",
    "●Normal - 99 mg/dL or below  == [0]\n",
    "\n",
    "●Prediabetes - 100 – 125 mg/dL == [1]\n",
    "\n",
    "●Diabetes - 126 mg/dL or above  == [2]\n",
    "\n",
    "\n",
    "outlires : \n",
    "[1975.0, 9091.0]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU3d5jTOmPmP"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvkRZg4WmPmP"
   },
   "outputs": [],
   "source": [
    "c1 = np.reshape(x[:,1], (-1, 1))\n",
    "c1[c1 == 1975.0] = np.nan\n",
    "c1[c1 == 9091.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ub3F0qJmPmQ"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx4IyjsemPmQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c1 = imputer.fit_transform(c1)\n",
    "x[:,1] = c1[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qzC3APWmPmR"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFwUPnT6mPmR"
   },
   "outputs": [],
   "source": [
    "c1[c1 < 100 ] = 0\n",
    "c1[np.logical_and(c1 < 126, c1 >= 100)] = 1\n",
    "c1[ c1 >= 126 ] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfklug_jmPmS"
   },
   "outputs": [],
   "source": [
    "x[:,1] = c1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39bQke1Ionih",
    "outputId": "96f27b7f-b6dd-419e-d88a-ad69916537ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 2., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 2., 0.,\n",
       "       0., 1., 2., 2., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 2., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 2., 0., 1., 1., 0., 0., 2., 0., 0., 1., 0., 0., 0., 2., 2., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 1., 2., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 2., 2., 0., 0., 1., 0., 1., 0., 2., 0., 2., 2., 0., 0., 0.,\n",
       "       0., 0., 1., 2., 2., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       2., 2., 0., 0., 0., 0., 2., 1., 0., 0., 0., 0., 1., 2., 0., 1., 2.,\n",
       "       2., 0., 0., 2., 0., 0., 1., 0., 0., 2., 1., 0., 2., 0., 2., 1., 1.,\n",
       "       2., 0., 0., 0., 1., 1., 0., 2., 2., 2., 0., 0., 1., 0., 0., 0., 2.,\n",
       "       2., 2., 0., 1., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 2.,\n",
       "       0., 1., 2., 0., 2., 1., 0., 0., 0., 2., 2., 0., 0., 0., 1., 2., 1.,\n",
       "       2., 0., 0., 1., 1., 1., 2., 0., 1., 0., 0., 0., 2., 1., 0., 0., 1.,\n",
       "       2., 1., 0., 0., 0., 1., 1., 0., 0., 2., 0., 2., 1., 0., 2., 1., 1.,\n",
       "       0., 0., 0., 2., 0., 2., 0., 2., 1., 0., 1., 1., 2., 0., 2., 0., 2.,\n",
       "       0., 0., 1., 1., 1., 2., 0., 1., 0., 0., 0., 2., 1., 0., 0., 1., 2.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 2., 0., 2., 1., 0., 2., 1., 1., 0.,\n",
       "       0., 0., 2., 0., 2., 0., 0., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 2., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3REjx4z3o8Wh"
   },
   "source": [
    "# imputing column 3 (HDL Chol (mg/dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HqnsMkNo8Wp"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 1\n",
    "\n",
    "standard range :\n",
    "\n",
    "classification :\n",
    "\n",
    "\n",
    "● Desirable - 60 mg/dL or above  == [0]\n",
    "\n",
    "● Borderline - 35 – 60 mg/dL == [1]\n",
    "\n",
    "● High risk - 35 mg/dL or below  == [2]\n",
    "\n",
    "\n",
    "outlires : \n",
    "[1.0]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJm-GINro8Wp"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgkwwDR3o8Wp"
   },
   "outputs": [],
   "source": [
    "c2 = np.reshape(x[:,2], (-1, 1))\n",
    "c2[c2 == 1.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYJAV68Eo8Wp"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKJzLkbzo8Wp"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c2 = imputer.fit_transform(c2)\n",
    "x[:,2] = c2[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OMPbhF2o8Wp"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUJiA4iTo8Wp"
   },
   "outputs": [],
   "source": [
    "c2[ c2 <= 35 ] = 2        # Start with x<=30 and rank 2\n",
    "c2[np.logical_and(c2 < 60, c2 > 35)] = 1\n",
    "c2[c2 >= 60 ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHcS405oo8Wq"
   },
   "outputs": [],
   "source": [
    "x[:,2] = c2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tw05bkbo8Wq",
    "outputId": "0d7fec31-d998-44b4-c937-3f390df1588c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 2., 1., 1., 2., 1., 1., 0., 1., 1.,\n",
       "       2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       2., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 2., 1., 1., 1., 2., 0., 1., 2., 0., 1., 2., 1., 1., 2., 1.,\n",
       "       2., 0., 1., 1., 2., 1., 1., 2., 1., 1., 2., 1., 2., 1., 1., 2., 1.,\n",
       "       1., 1., 2., 1., 1., 0., 1., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 2., 1., 1., 2., 1., 2., 1., 1., 0., 0., 1., 1., 0., 1., 0., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 2., 1., 0., 1., 0., 1., 1., 0., 2., 1., 2.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 0., 1.,\n",
       "       1., 0., 2., 1., 1., 1., 0., 1., 1., 1., 2., 2., 2., 2., 0., 1., 0.,\n",
       "       2., 1., 1., 1., 2., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       2., 1., 1., 0., 1., 1., 2., 0., 1., 0., 1., 1., 1., 0., 2., 1., 2.,\n",
       "       2., 1., 1., 1., 1., 2., 1., 1., 2., 1., 2., 1., 1., 1., 2., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 0., 1., 2., 1., 1., 1., 1., 1., 0., 2.,\n",
       "       2., 0., 2., 1., 1., 1., 2., 1., 2., 0., 0., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 2., 2., 2., 1., 1., 0., 1., 2., 2., 2., 0., 1., 1., 0., 2., 0.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 2., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 2., 1., 1., 1., 0., 1., 1., 2., 1., 1., 0., 0., 0., 1.,\n",
       "       2., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 2., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 2., 1., 1., 1., 1., 2., 1., 1., 0., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 2., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyqdEoWTvyvM"
   },
   "source": [
    "# imputing column 4 (TChol/HDL ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-wiEcITvyvT"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 1\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "\n",
    "● Desirable - 4.0 or below  == [0]\n",
    "\n",
    "● Borderline - 4-6  == [1]\n",
    "\n",
    "● High risk - 6.0 or above  == [2]\n",
    "\n",
    "\n",
    "outlires : \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlgSpTUuvyvT"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QybfqRYjvyvT"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJRIRq99vyvT"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c3 = np.reshape(x[:,3], (-1, 1))\n",
    "c3 = imputer.fit_transform(c3)\n",
    "x[:,3] = c3[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UuwZch_vyvT"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bePsi6KcvyvT"
   },
   "outputs": [],
   "source": [
    "c3[ c3 <= 4 ] = 0\n",
    "c3[np.logical_and(c3 < 6, c3 > 4)] = 1\n",
    "c3[c3 >= 6 ] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykuFywllvyvU"
   },
   "outputs": [],
   "source": [
    "x[:,3] = c3[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtRKrLU7vyvU",
    "outputId": "73958e5b-102b-4daa-f6d1-92d99d73a378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 2., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 2., 0., 0., 1., 2., 1., 0., 0., 0., 0., 2., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 2., 0., 0., 0.,\n",
       "       1., 2., 1., 1., 0., 0., 0., 2., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 2., 0., 1., 0., 2., 0., 1., 1., 0., 0., 2., 1., 2., 2., 1.,\n",
       "       2., 0., 1., 1., 1., 0., 1., 2., 1., 1., 1., 1., 1., 1., 0., 2., 0.,\n",
       "       1., 2., 2., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 2., 1., 2., 1., 0., 0., 0., 1., 0., 0., 0., 1., 2.,\n",
       "       2., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 2., 2., 0., 1., 2., 1., 0., 1., 0., 2., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 1., 2., 1., 0., 0., 0., 1., 2., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 2., 0., 0., 1., 0., 1., 1., 2., 1., 0., 1., 0.,\n",
       "       2., 1., 2., 0., 2., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 2., 0.,\n",
       "       2., 2., 1., 0., 1., 1., 2., 0., 0., 0., 0., 2., 0., 0., 2., 0., 2.,\n",
       "       2., 0., 0., 0., 2., 2., 0., 2., 2., 0., 1., 1., 1., 0., 2., 1., 0.,\n",
       "       2., 0., 1., 1., 1., 1., 0., 0., 2., 1., 0., 0., 2., 1., 1., 0., 2.,\n",
       "       2., 0., 1., 2., 0., 1., 1., 0., 1., 0., 0., 1., 2., 1., 2., 0., 2.,\n",
       "       0., 2., 2., 2., 1., 1., 0., 1., 1., 1., 2., 0., 1., 2., 0., 2., 0.,\n",
       "       1., 2., 1., 0., 2., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 2., 2., 1., 0., 0., 2., 1., 2., 1., 1., 0., 0., 0., 1.,\n",
       "       2., 1., 0., 2., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 2., 2., 1., 0., 0., 2., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 0., 2., 0., 0., 0., 1., 2., 1., 1.,\n",
       "       0., 0., 0., 2., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 2., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 2., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 2., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-4GmEaGyRxn"
   },
   "source": [
    "# imputing column 5 (Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwvCzwUzyRx5"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 0\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "\n",
    "● Desirable - 20–39  == [0]\n",
    "\n",
    "● Borderline - 40–64 == [1]\n",
    "\n",
    "● High risk - 65 or above  == [2]\n",
    "\n",
    "\n",
    "outlires : \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMAci27zyRx5"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUmbzFIdyRx6"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIgPkSBxyRx6"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-Bzs023yRx6"
   },
   "outputs": [],
   "source": [
    "c4 = np.reshape(x[:,4], (-1, 1))\n",
    "\n",
    "c4[ c4 < 40 ] = 0\n",
    "c4[np.logical_and(c4 < 65, c4 >= 40)] = 1\n",
    "c4[c4 >= 65 ] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buOxR5VsyRx6"
   },
   "outputs": [],
   "source": [
    "x[:,4] = c4[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_84qTGeyRx7",
    "outputId": "80eeeaa4-8e20-4c74-d798-55695c31393e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdZvFqVv2GBB"
   },
   "source": [
    "# imputing column 6 (Gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aclvg4sD2GBH"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 5\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "outlires : \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAsNzMIg2GBH"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5mohdS92GBH"
   },
   "outputs": [],
   "source": [
    "c5 = np.reshape(x[:,5], (-1, 1))\n",
    "c5[c5 == 2.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KagYBVs2GBH"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfHNl-D22GBH"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan , strategy = 'most_frequent')\n",
    "imputer.fit(c5)\n",
    "c5 = imputer.transform(c5)\n",
    "x[:,5] = c5[:,0]\n",
    "\n",
    "#female == 0\n",
    "#male ==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lSJiuZg6Q8jB",
    "outputId": "d5c82cef-bcb4-4fd1-ff45-487cfc6c9de1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwBPvf_7Sscq"
   },
   "source": [
    "# imputing column 7 (Height /stature (cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFgYvt_PSsdB"
   },
   "source": [
    "zero values : 2\n",
    "\n",
    "null values : 0\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification : x\n",
    "\n",
    "outlires : [0.0, 0.0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbRU45J6SsdC"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAYZlHl3T7y-"
   },
   "outputs": [],
   "source": [
    "c6 = np.reshape(x[:,6], (-1, 1))\n",
    "c6[c6 == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEJy2I-SSsdC"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvnK43_SSsdD",
    "outputId": "ad7409a1-757c-494c-9670-b6c4e721e861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([154.9       , 152.4       , 170.2       , 162.6       ,\n",
       "       177.8       , 162.6       , 157.5       , 182.9       ,\n",
       "       170.2       , 147.3       , 177.8       , 172.7       ,\n",
       "       165.1       , 160.        , 175.3       , 160.        ,\n",
       "       180.3       , 154.9       , 160.        , 162.6       ,\n",
       "       180.3       , 157.5       , 167.6       , 165.1       ,\n",
       "       180.3       , 154.9       , 182.9       , 165.1       ,\n",
       "       193.        , 175.3       , 154.9       , 167.6       ,\n",
       "       160.        , 152.4       , 180.3       , 157.5       ,\n",
       "       185.4       , 152.4       , 182.9       , 182.9       ,\n",
       "       177.8       , 160.        , 165.1       , 152.4       ,\n",
       "       165.1       , 170.2       , 175.3       , 170.2       ,\n",
       "       182.9       , 160.        , 170.2       , 172.7       ,\n",
       "       175.3       , 170.2       , 160.        , 162.6       ,\n",
       "       162.6       , 172.7       , 172.7       , 165.1       ,\n",
       "       157.5       , 162.6       , 162.6       , 172.7       ,\n",
       "       175.3       , 157.5       , 160.        , 167.6       ,\n",
       "       182.9       , 157.5       , 165.1       , 175.3       ,\n",
       "       182.9       , 182.9       , 167.6       , 157.5       ,\n",
       "       165.1       , 170.2       , 170.2       , 167.6       ,\n",
       "       170.2       , 180.3       , 157.5       , 165.1       ,\n",
       "       157.5       , 160.        , 177.8       , 165.1       ,\n",
       "       157.5       , 175.3       , 167.6       , 175.3       ,\n",
       "       160.        , 182.9       , 165.1       , 180.3       ,\n",
       "       185.4       , 152.4       , 160.        , 180.3       ,\n",
       "       167.6       , 185.4       , 182.9       , 188.        ,\n",
       "       175.3       , 180.3       , 170.2       , 162.6       ,\n",
       "       175.3       , 167.6       , 162.6       , 160.        ,\n",
       "       175.3       , 177.8       , 175.3       , 162.6       ,\n",
       "       177.8       , 167.6       , 149.9       , 180.3       ,\n",
       "       149.9       , 162.6       , 162.6       , 170.2       ,\n",
       "       167.6       , 177.8       , 172.7       , 162.6       ,\n",
       "       154.9       , 152.4       , 175.3       , 175.3       ,\n",
       "       170.2       , 152.4       , 147.3       , 172.7       ,\n",
       "       160.        , 162.6       , 172.7       , 175.3       ,\n",
       "       167.6       , 152.4       , 157.5       , 172.7       ,\n",
       "       132.1       , 157.5       , 177.8       , 157.5       ,\n",
       "       162.6       , 182.9       , 180.3       , 149.9       ,\n",
       "       160.        , 172.7       , 182.9       , 185.4       ,\n",
       "       165.1       , 149.9       , 170.2       , 157.5       ,\n",
       "       175.3       , 165.1       , 182.9       , 180.3       ,\n",
       "       160.        , 160.        , 149.9       , 154.9       ,\n",
       "       170.2       , 157.5       , 167.6       , 165.1       ,\n",
       "       190.5       , 170.2       , 165.1       , 154.9       ,\n",
       "       157.5       , 167.6       , 162.6       , 175.3       ,\n",
       "       170.2       , 142.2       , 157.5       , 162.6       ,\n",
       "       162.6       , 180.3       , 162.6       , 157.5       ,\n",
       "       165.1       , 172.7       , 180.3       , 162.6       ,\n",
       "       157.5       , 162.6       , 170.2       , 167.6       ,\n",
       "       175.3       , 170.2       , 162.6       , 175.3       ,\n",
       "       170.2       , 175.3       , 170.2       , 182.9       ,\n",
       "       157.5       , 160.        , 157.5       , 167.6       ,\n",
       "       170.2       , 162.6       , 172.7       , 165.1       ,\n",
       "       160.        , 185.4       , 160.        , 172.7       ,\n",
       "       177.8       , 177.8       , 160.        , 172.7       ,\n",
       "       172.7       , 170.2       , 157.5       , 180.3       ,\n",
       "       157.5       , 180.3       , 165.1       , 170.2       ,\n",
       "       162.6       , 180.3       , 154.9       , 177.8       ,\n",
       "       177.8       , 180.3       , 175.3       , 165.1       ,\n",
       "       160.        , 167.6       , 165.1       , 165.1       ,\n",
       "       170.2       , 167.6       , 177.8       , 157.5       ,\n",
       "       180.3       , 177.8       , 172.7       , 175.3       ,\n",
       "       162.6       , 190.5       , 157.5       , 162.6       ,\n",
       "       162.6       , 160.        , 154.9       , 175.3       ,\n",
       "       175.3       , 167.6       , 175.3       , 165.1       ,\n",
       "       165.1       , 170.2       , 160.        , 167.6       ,\n",
       "       177.8       , 167.6       , 160.        , 170.2       ,\n",
       "       190.5       , 172.7       , 188.        , 165.1       ,\n",
       "       175.3       , 185.4       , 180.3       , 154.9       ,\n",
       "       154.9       , 177.8       , 167.6       , 160.        ,\n",
       "       170.2       , 175.3       , 185.4       , 160.        ,\n",
       "       157.5       , 167.6       , 167.6       , 170.2       ,\n",
       "       167.6       , 172.7       , 177.8       , 157.5       ,\n",
       "       165.1       , 175.3       , 154.9       , 160.        ,\n",
       "       165.1       , 172.7       , 170.2       , 149.9       ,\n",
       "       165.1       , 170.2       , 160.        , 175.3       ,\n",
       "       188.        , 160.        , 188.        , 172.7       ,\n",
       "       167.6       , 193.        , 160.        , 165.1       ,\n",
       "       175.3       , 177.8       , 185.4       , 177.8       ,\n",
       "       162.6       , 147.3       , 162.6       , 175.3       ,\n",
       "       154.9       , 160.        , 165.1       , 157.5       ,\n",
       "       172.7       , 180.3       , 172.7       , 160.        ,\n",
       "       160.        , 157.5       , 154.9       , 175.3       ,\n",
       "       177.8       , 170.2       , 167.6       , 188.        ,\n",
       "       157.5       , 162.6       , 172.7       , 167.6       ,\n",
       "       170.2       , 170.2       , 170.2       , 172.7       ,\n",
       "       172.7       , 160.        , 170.2       , 139.7       ,\n",
       "       154.9       , 149.9       , 167.6       , 170.2       ,\n",
       "       175.3       , 152.4       , 160.        , 175.3       ,\n",
       "       160.        , 160.        , 165.1       , 154.9       ,\n",
       "       149.9       , 177.8       , 165.1       , 162.6       ,\n",
       "       175.3       , 157.5       , 160.        , 175.3       ,\n",
       "       172.7       , 167.6       , 165.1       , 154.9       ,\n",
       "       162.6       , 152.4       , 165.1       , 167.6       ,\n",
       "       160.        , 167.6       , 165.1       , 177.8       ,\n",
       "       180.3       , 157.5       , 162.6       , 160.        ,\n",
       "       167.6       , 149.9       , 152.4       , 154.9       ,\n",
       "       154.9       , 157.5       , 170.2       , 170.2       ,\n",
       "       170.2       , 172.7       , 172.7       , 160.        ,\n",
       "       170.2       , 139.7       , 154.9       , 149.9       ,\n",
       "       167.6       , 170.2       , 175.3       , 152.4       ,\n",
       "       160.        , 175.3       , 160.        , 160.        ,\n",
       "       165.1       , 154.9       , 149.9       , 177.8       ,\n",
       "       165.1       , 162.6       , 175.3       , 157.5       ,\n",
       "       160.        , 175.3       , 172.7       , 167.6       ,\n",
       "       165.1       , 154.9       , 162.6       , 152.4       ,\n",
       "       165.1       , 167.6       , 160.        , 167.6       ,\n",
       "       165.1       , 177.8       , 180.3       , 162.6       ,\n",
       "       162.6       , 172.7       , 175.3       , 157.5       ,\n",
       "       160.        , 167.6       , 182.9       , 157.5       ,\n",
       "       165.1       , 175.3       , 182.9       , 182.9       ,\n",
       "       167.6       , 157.5       , 165.1       , 170.2       ,\n",
       "       170.2       , 167.6       , 170.2       , 180.3       ,\n",
       "       157.5       , 165.1       , 157.5       , 160.        ,\n",
       "       177.8       , 165.1       , 157.5       , 175.3       ,\n",
       "       167.6       , 175.3       , 167.60502008, 182.9       ,\n",
       "       165.1       , 180.3       , 185.4       , 152.4       ,\n",
       "       160.        , 180.3       , 167.6       , 167.60502008,\n",
       "       182.9       , 188.        , 175.3       , 180.3       ,\n",
       "       170.2       , 160.        , 162.6       , 180.3       ,\n",
       "       157.5       , 167.6       , 165.1       , 180.3       ,\n",
       "       154.9       , 182.9       , 165.1       , 193.        ,\n",
       "       175.3       , 154.9       , 167.6       , 160.        ,\n",
       "       152.4       , 180.3       , 157.5       , 185.4       ,\n",
       "       152.4       , 182.9       , 182.9       , 177.8       ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c6 = np.reshape(x[:,6], (-1, 1))\n",
    "c6 = imputer.fit_transform(c6)\n",
    "x[:,6] = c6[:,0]\n",
    "x[:,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPr6GCXEV1fN"
   },
   "source": [
    "# imputing column 8 (weight 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hobj6n-CV1fO"
   },
   "source": [
    "zero values : 2\n",
    "\n",
    "null values : 0\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification : x\n",
    "\n",
    "outlires : \n",
    "[0.0, 322.34, 996.98, 0.0]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvB_04xsV1fO"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ih1IyijV1fP"
   },
   "outputs": [],
   "source": [
    "c7 = np.reshape(x[:,7], (-1, 1))\n",
    "c7[c7 == 0.0] = np.nan\n",
    "c7[c7 == 322.34] = np.nan\n",
    "c7[c7 == 996.98] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkrytXElV1fP"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRmbI8g6V1fP"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c7 = imputer.fit_transform(c7)\n",
    "x[:,7] = c7[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTdHTTCSV1fQ",
    "outputId": "f7b35dcc-05ec-4afa-cc70-a0a304660269"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54.93      ,  98.97      , 116.22      ,  54.03      ,\n",
       "        83.08      ,  86.26      ,  86.71      ,  77.18      ,\n",
       "        75.36      ,  91.71      ,  70.82      ,  88.53      ,\n",
       "        77.18      ,  74.91      ,  83.08      ,  71.28      ,\n",
       "        83.08      ,  72.19      ,  57.2       ,  88.98      ,\n",
       "        80.81      , 104.42      , 130.75      ,  83.99      ,\n",
       "        51.3       ,  53.57      , 114.41      ,  45.4       ,\n",
       "        65.83      ,  85.81      ,  79.        ,  63.11      ,\n",
       "        86.71      ,  79.        ,  53.57      ,  84.44      ,\n",
       "        72.19      ,  93.07      ,  65.83      ,  97.16      ,\n",
       "        72.64      ,  77.18      ,  87.17      ,  62.2       ,\n",
       "       105.78      ,  74.91      ,  88.98      ,  81.72      ,\n",
       "        99.43      ,  69.92      ,  82.17      ,  84.9       ,\n",
       "        75.82      ,  99.88      , 124.4       ,  81.72      ,\n",
       "        98.52      ,  85.81      ,  83.08      ,  73.09      ,\n",
       "       106.69      ,  56.75      ,  57.2       ,  62.2       ,\n",
       "        74.91      ,  46.31      ,  57.66      ,  97.16      ,\n",
       "       111.23      ,  73.09      , 119.86      ,  64.47      ,\n",
       "        64.92      ,  83.08      ,  78.54      , 101.24      ,\n",
       "        69.92      ,  99.43      ,  76.73      ,  90.8       ,\n",
       "        90.8       ,  65.83      ,  80.81      ,  97.61      ,\n",
       "        93.07      ,  68.55      ,  77.18      ,  76.73      ,\n",
       "        72.19      ,  49.94      ,  89.89      ,  83.99      ,\n",
       "        64.47      ,  63.11      ,  70.82      ,  99.88      ,\n",
       "        89.44      ,  90.8       ,  69.92      ,  92.16      ,\n",
       "        81.72      ,  68.1       ,  92.62      ,  90.8       ,\n",
       "        56.75      ,  74.91      ,  96.25      , 103.06      ,\n",
       "        68.1       ,  79.        ,  54.03      ,  79.45      ,\n",
       "       119.4       ,  70.82      ,  54.48      ,  78.09      ,\n",
       "        77.18      ,  71.73      ,  74.46      ,  76.73      ,\n",
       "       106.69      ,  56.75      , 110.78      , 102.15      ,\n",
       "        63.56      , 103.06      ,  72.64      ,  75.82      ,\n",
       "       147.55      ,  54.93      ,  68.55      , 101.24      ,\n",
       "       120.76      ,  80.36      ,  77.18      ,  66.28      ,\n",
       "        54.93      ,  77.18      ,  68.55      ,  72.19      ,\n",
       "        47.67      , 125.76      ,  72.64      ,  65.83      ,\n",
       "       145.28      ,  74.        ,  74.        ,  76.73      ,\n",
       "       105.33      ,  95.34      ,  72.64      ,  65.83      ,\n",
       "        97.61      , 115.77      , 139.83      ,  80.36      ,\n",
       "        71.73      ,  95.34      ,  55.84      ,  53.57      ,\n",
       "        75.82      ,  84.44      ,  71.73      ,  65.83      ,\n",
       "        54.03      , 128.03      ,  77.63      ,  78.09      ,\n",
       "        62.65      ,  84.9       ,  85.81      ,  92.62      ,\n",
       "        97.61      ,  75.82      ,  85.81      ,  81.72      ,\n",
       "        74.91      ,  81.27      ,  92.62      , 105.78      ,\n",
       "        95.34      ,  88.53      ,  90.35      ,  83.99      ,\n",
       "        66.74      ,  54.03      ,  83.54      ,  71.73      ,\n",
       "        59.02      ,  60.84      , 113.95      ,  90.8       ,\n",
       "        63.56      ,  51.76      ,  94.89      ,  95.34      ,\n",
       "        81.27      ,  49.49      ,  59.02      ,  65.83      ,\n",
       "        75.82      ,  81.27      ,  65.38      ,  59.02      ,\n",
       "        74.46      ,  91.25      ,  84.44      ,  79.        ,\n",
       "        61.74      ,  47.67      ,  56.3       ,  77.18      ,\n",
       "        60.84      ,  74.91      ,  86.71      ,  79.45      ,\n",
       "        81.72      ,  64.47      ,  66.74      ,  49.94      ,\n",
       "        92.62      ,  82.17      ,  84.9       ,  86.26      ,\n",
       "        82.17      ,  63.56      ,  91.25      ,  88.98      ,\n",
       "        69.46      ,  77.18      ,  85.35      ,  81.27      ,\n",
       "       117.59      ,  90.8       ,  73.55      ,  64.01      ,\n",
       "        83.08      ,  72.64      ,  54.48      ,  65.83      ,\n",
       "        79.        , 114.41      ,  61.29      ,  70.37      ,\n",
       "        81.27      ,  95.79      ,  52.21      ,  86.26      ,\n",
       "       131.66      ,  76.27      , 115.77      ,  93.07      ,\n",
       "       118.04      , 113.5       ,  75.36      ,  77.18      ,\n",
       "        82.63      ,  79.9       ,  65.83      ,  78.09      ,\n",
       "       125.76      ,  75.82      ,  93.07      ,  83.08      ,\n",
       "        55.84      ,  83.08      ,  44.95      , 122.58      ,\n",
       "        62.65      , 129.39      ,  81.72      ,  72.64      ,\n",
       "        77.18      ,  83.99      ,  74.        ,  84.9       ,\n",
       "        58.11      ,  69.46      ,  56.75      ,  70.37      ,\n",
       "       101.24      ,  73.09      ,  98.06      ,  81.27      ,\n",
       "       103.06      ,  72.19      ,  77.18      ,  62.65      ,\n",
       "        51.76      , 108.51      ,  79.        ,  85.35      ,\n",
       "        89.89      ,  51.76      , 102.15      ,  64.92      ,\n",
       "        66.28      ,  64.01      ,  68.55      , 112.59      ,\n",
       "        69.01      ,  59.02      ,  74.91      ,  81.72      ,\n",
       "        74.        ,  81.27      ,  70.82      ,  59.02      ,\n",
       "        72.64      ,  95.34      ,  74.46      ,  52.21      ,\n",
       "        72.19      ,  64.01      ,  76.73      ,  82.17      ,\n",
       "        81.72      , 107.6       ,  74.        ,  83.99      ,\n",
       "        81.72      , 111.23      ,  68.1       ,  66.28      ,\n",
       "        65.83      ,  64.47      ,  89.89      ,  67.19      ,\n",
       "        90.8       ,  86.26      ,  82.63      ,  99.88      ,\n",
       "        81.27      ,  96.25      ,  74.91      , 116.68      ,\n",
       "        83.54      ,  83.08      ,  98.97      ,  81.27      ,\n",
       "       103.51      , 131.21      ,  69.46      , 106.69      ,\n",
       "        65.38      ,  83.08      ,  69.92      ,  98.06      ,\n",
       "        82.17      ,  91.71      ,  72.64      ,  55.84      ,\n",
       "        89.44      ,  87.17      ,  84.9       ,  96.25      ,\n",
       "        84.44      ,  73.55      ,  54.48      ,  69.01      ,\n",
       "        95.34      ,  67.19      ,  77.18      ,  71.28      ,\n",
       "        58.57      ,  95.79      ,  85.81      ,  54.48      ,\n",
       "        54.93      ,  54.48      ,  76.73      ,  84.44      ,\n",
       "       118.95      , 100.79      , 100.79      ,  81.27      ,\n",
       "       101.7       ,  74.91      ,  83.99      ,  80.46905242,\n",
       "        80.36      ,  65.83      ,  66.28      ,  69.92      ,\n",
       "        61.74      ,  52.21      ,  78.54      ,  69.92      ,\n",
       "        75.82      ,  89.44      , 103.51      , 131.21      ,\n",
       "        69.46      , 106.69      ,  65.38      ,  83.08      ,\n",
       "        69.92      ,  98.06      ,  82.17      ,  91.71      ,\n",
       "        72.64      ,  55.84      ,  89.44      ,  87.17      ,\n",
       "        84.9       ,  96.25      ,  84.44      ,  73.55      ,\n",
       "        54.48      ,  69.01      ,  95.34      ,  67.19      ,\n",
       "        77.18      ,  71.28      ,  58.57      ,  95.79      ,\n",
       "        85.81      ,  54.48      ,  54.93      ,  54.48      ,\n",
       "        76.73      ,  84.44      , 118.95      , 100.79      ,\n",
       "       100.79      ,  81.27      , 101.7       ,  74.91      ,\n",
       "        83.99      ,  66.74      ,  80.36      ,  56.75      ,\n",
       "        57.2       ,  62.2       ,  74.91      ,  46.31      ,\n",
       "        57.66      ,  97.16      , 111.23      ,  73.09      ,\n",
       "       119.86      ,  64.47      ,  64.92      ,  83.08      ,\n",
       "        78.54      , 101.24      ,  69.92      ,  99.43      ,\n",
       "        76.73      ,  90.8       ,  90.8       ,  65.83      ,\n",
       "        80.81      ,  97.61      ,  93.07      ,  68.55      ,\n",
       "        77.18      ,  76.73      ,  72.19      ,  80.46905242,\n",
       "        89.89      ,  83.99      ,  64.47      ,  63.11      ,\n",
       "        70.82      ,  99.88      ,  89.44      ,  90.8       ,\n",
       "        69.92      ,  92.16      ,  81.72      ,  68.1       ,\n",
       "        92.62      ,  90.8       ,  56.75      ,  74.91      ,\n",
       "        96.25      ,  57.2       ,  80.46905242,  80.46905242,\n",
       "       104.42      , 130.75      ,  83.99      ,  51.3       ,\n",
       "        53.57      , 114.41      ,  45.4       ,  65.83      ,\n",
       "        85.81      ,  79.        ,  63.11      ,  86.71      ,\n",
       "        79.        ,  53.57      ,  84.44      ,  72.19      ,\n",
       "        93.07      ,  65.83      ,  97.16      ,  72.64      ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKlmXbQVbtj1"
   },
   "source": [
    "# imputing column 9 (weight 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLRs-U9GbtkL"
   },
   "source": [
    "zero values : 2\n",
    "\n",
    "null values : 0\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification : x\n",
    "\n",
    "outlires : \n",
    "[0.0, 0.0]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IFR5xh4btkM"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58YZjdslbtkM"
   },
   "outputs": [],
   "source": [
    "c8 = np.reshape(x[:,8], (-1, 1))\n",
    "c8[c8 == 0.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8afs8NKubtkN"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIItdFi6btkN"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c8 = imputer.fit_transform(c8)\n",
    "x[:,8] = c8[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDRkC4QfbtkO",
    "outputId": "47777f51-a1b0-4d39-b249-37a3bd42edc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54.        ,  61.        ,  85.        ,  52.        ,\n",
       "        64.        ,  73.        ,  52.        ,  66.        ,\n",
       "        72.        ,  77.        ,  85.        , 124.        ,\n",
       "        77.        ,  64.        ,  70.        ,  72.        ,\n",
       "        74.        , 100.        ,  52.        ,  67.        ,\n",
       "       101.        ,  62.        ,  73.        , 107.        ,\n",
       "       126.        ,  84.        ,  82.        ,  83.        ,\n",
       "        74.        , 111.        ,  51.        ,  54.        ,\n",
       "       111.        ,  70.        ,  74.        ,  79.        ,\n",
       "        79.        ,  59.        , 103.        , 118.        ,\n",
       "        68.        ,  54.        ,  66.        ,  77.        ,\n",
       "        95.        ,  95.        , 131.        ,  84.        ,\n",
       "        64.        ,  81.        ,  82.        ,  93.        ,\n",
       "        59.        ,  70.        ,  91.        ,  57.        ,\n",
       "        91.        , 102.        ,  77.        ,  56.        ,\n",
       "        45.        ,  54.        ,  99.        ,  82.        ,\n",
       "        76.        ,  64.        ,  65.        , 117.        ,\n",
       "        75.        ,  57.        ,  75.        ,  87.        ,\n",
       "        82.        , 114.        ,  86.        ,  66.        ,\n",
       "        54.        , 103.        ,  73.        ,  66.        ,\n",
       "        91.        ,  96.        , 108.        ,  69.        ,\n",
       "        81.        , 114.        ,  96.        ,  71.        ,\n",
       "        84.        , 140.        ,  54.        ,  74.        ,\n",
       "        77.        ,  77.        ,  73.        ,  86.        ,\n",
       "        99.        ,  75.        ,  74.        ,  95.        ,\n",
       "        72.        ,  81.        ,  86.        ,  77.        ,\n",
       "        77.        ,  83.        ,  87.        ,  84.        ,\n",
       "        83.        , 116.        ,  62.        ,  57.        ,\n",
       "        68.        ,  73.        ,  93.        ,  57.        ,\n",
       "       126.        ,  81.        ,  77.        ,  89.        ,\n",
       "        67.        ,  66.        , 106.        ,  96.        ,\n",
       "        62.        , 119.        , 114.        ,  73.        ,\n",
       "        92.        ,  57.        ,  76.        ,  93.        ,\n",
       "       101.        ,  63.        ,  89.        ,  77.        ,\n",
       "        82.        , 113.        ,  66.        , 131.        ,\n",
       "       116.        ,  65.        ,  48.        ,  81.        ,\n",
       "        85.        ,  59.        ,  86.        ,  69.        ,\n",
       "        66.        , 101.        ,  97.        ,  75.        ,\n",
       "       129.        , 120.        , 123.        ,  91.        ,\n",
       "        82.        ,  93.        ,  83.        ,  84.        ,\n",
       "        70.        ,  85.        ,  54.        ,  71.        ,\n",
       "        69.        ,  89.        ,  63.        ,  50.        ,\n",
       "        87.        ,  73.        ,  84.        ,  75.        ,\n",
       "        84.        , 107.        ,  79.        ,  71.        ,\n",
       "        64.        ,  69.        , 109.        ,  95.        ,\n",
       "        98.        ,  54.        ,  66.        ,  64.        ,\n",
       "       148.        ,  81.        ,  82.        ,  74.        ,\n",
       "        90.        ,  71.        ,  76.        ,  73.        ,\n",
       "        72.        ,  86.        ,  91.        ,  52.        ,\n",
       "        86.        ,  67.        ,  76.        ,  75.        ,\n",
       "        69.        ,  93.        ,  99.        ,  93.        ,\n",
       "        55.        ,  81.        ,  83.        ,  84.        ,\n",
       "       104.        ,  56.        ,  64.        , 105.        ,\n",
       "        71.        , 118.        ,  77.        ,  45.        ,\n",
       "        79.        ,  54.        ,  55.        ,  66.        ,\n",
       "        89.        ,  82.        ,  93.        , 121.        ,\n",
       "        86.        ,  82.        ,  83.        ,  64.        ,\n",
       "       104.        ,  77.        ,  67.        , 119.        ,\n",
       "        98.        , 145.        ,  78.        ,  66.        ,\n",
       "        70.        ,  50.        ,  91.        ,  66.        ,\n",
       "       128.        ,  98.        , 107.        ,  77.        ,\n",
       "        72.        ,  54.        ,  63.        ,  69.        ,\n",
       "        90.        ,  64.        ,  67.        ,  81.        ,\n",
       "        79.        ,  83.        ,  66.        ,  98.        ,\n",
       "        79.        ,  77.        ,  59.        ,  63.        ,\n",
       "        79.        ,  75.        ,  52.        ,  92.        ,\n",
       "        64.        ,  99.        ,  92.        , 101.        ,\n",
       "        84.        ,  83.        ,  69.        ,  89.        ,\n",
       "        72.        ,  74.        ,  66.        ,  68.        ,\n",
       "       116.        ,  96.        ,  98.        ,  70.        ,\n",
       "        80.        ,  91.        ,  85.        ,  80.        ,\n",
       "        80.        ,  63.        ,  86.        ,  78.        ,\n",
       "        77.        ,  77.        ,  74.        ,  61.        ,\n",
       "        71.        ,  84.        ,  58.        ,  55.        ,\n",
       "        76.        , 102.        ,  89.        ,  56.        ,\n",
       "       100.        ,  77.        ,  90.        ,  79.        ,\n",
       "        69.        ,  65.        ,  87.        ,  82.        ,\n",
       "       114.        ,  73.        , 132.        ,  89.        ,\n",
       "        77.        ,  82.        ,  79.        ,  73.        ,\n",
       "        81.        ,  95.        , 101.        ,  75.        ,\n",
       "        91.        ,  72.        ,  91.        ,  76.        ,\n",
       "       103.        , 102.        ,  83.        ,  72.        ,\n",
       "       106.        ,  69.        ,  85.        ,  83.        ,\n",
       "        82.        ,  89.        ,  55.        ,  84.        ,\n",
       "        84.        ,  59.        ,  95.        ,  86.        ,\n",
       "        66.        ,  54.        ,  81.        ,  90.        ,\n",
       "        76.        ,  56.        ,  72.        ,  59.        ,\n",
       "        54.        ,  81.        ,  91.        ,  81.        ,\n",
       "        97.        , 100.        ,  70.        ,  78.        ,\n",
       "       111.        ,  75.        ,  76.        ,  82.        ,\n",
       "        62.        ,  82.        ,  83.        ,  79.        ,\n",
       "        84.        ,  75.        ,  83.        ,  93.        ,\n",
       "        95.        ,  85.        ,  80.34939759,  46.        ,\n",
       "        48.        ,  65.        ,  49.        ,  66.        ,\n",
       "        73.        ,  78.        ,  61.        ,  75.        ,\n",
       "        96.        ,  74.        ,  72.        ,  77.        ,\n",
       "        74.        ,  57.        ,  87.        ,  52.        ,\n",
       "        58.        ,  99.        ,  66.        ,  54.        ,\n",
       "        81.        ,  90.        ,  76.        ,  56.        ,\n",
       "        72.        ,  59.        ,  54.        ,  81.        ,\n",
       "        91.        ,  81.        ,  97.        , 100.        ,\n",
       "        70.        ,  78.        , 111.        ,  75.        ,\n",
       "        76.        ,  82.        ,  62.        ,  82.        ,\n",
       "        83.        ,  79.        ,  84.        ,  75.        ,\n",
       "        83.        ,  93.        ,  95.        ,  85.        ,\n",
       "        70.        ,  46.        ,  48.        ,  65.        ,\n",
       "        49.        ,  66.        ,  73.        ,  78.        ,\n",
       "        61.        ,  75.        ,  96.        ,  54.        ,\n",
       "        99.        ,  82.        ,  76.        ,  64.        ,\n",
       "        65.        , 117.        ,  75.        ,  57.        ,\n",
       "        75.        ,  87.        ,  82.        , 114.        ,\n",
       "        86.        ,  66.        ,  54.        , 103.        ,\n",
       "        73.        ,  66.        ,  91.        ,  96.        ,\n",
       "       108.        ,  69.        ,  81.        , 114.        ,\n",
       "        96.        ,  71.        ,  84.        , 140.        ,\n",
       "        54.        ,  74.        ,  77.        ,  77.        ,\n",
       "        73.        ,  86.        ,  99.        ,  75.        ,\n",
       "        74.        ,  95.        ,  72.        ,  81.        ,\n",
       "        86.        ,  77.        ,  77.        ,  83.        ,\n",
       "        87.        ,  52.        ,  67.        , 101.        ,\n",
       "        62.        ,  73.        , 107.        , 126.        ,\n",
       "        84.        ,  82.        ,  83.        ,  74.        ,\n",
       "       111.        ,  51.        ,  54.        ,  80.34939759,\n",
       "        70.        ,  74.        ,  79.        ,  79.        ,\n",
       "        59.        , 103.        , 118.        ,  68.        ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaAzmKkMi3PE"
   },
   "source": [
    "# imputing column 10 (BMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVQaoAn2i3PL"
   },
   "source": [
    "zero values : 4\n",
    "\n",
    "null values : 0\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "\n",
    "● Under Weight - 18.5 below  == [0]\n",
    "\n",
    "● Normal - 18.5-24.9 == [1]\n",
    "\n",
    "● Over Weight - 25-29.9  == [2]\n",
    "\n",
    "● Obesity (Class I) - 30-34.9  == [3]\n",
    "\n",
    "● Obesity (Class II) - 35-39.9  == [4]\n",
    "\n",
    "● Extreme Obesity - 40 or above  == [5]\n",
    "\n",
    "\n",
    "outlires : \n",
    "[83.28, 104.94, 377.28]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDNV79_8i3PL"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC3ZWXpZi3PM"
   },
   "outputs": [],
   "source": [
    "c9 = np.reshape(x[:,9], (-1, 1))\n",
    "c9[c9 == 0.0] = np.nan\n",
    "c9[c9 == 377.28] = np.nan\n",
    "c9[c9 == 104.94] = np.nan\n",
    "c9[c9 == 83.28] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x90l7jUPi3PM"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQRyaLDji3PM"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c9 = imputer.fit_transform(c9)\n",
    "x[:,9] = c9[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeSgSCpKi3PM"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3agDmEOHi3PM"
   },
   "outputs": [],
   "source": [
    "c9[c9 < 18.5 ] = 0\n",
    "c9[np.logical_and(c9 < 25, c9 >= 18.5)] = 1\n",
    "c9[np.logical_and(c9 < 30, c9 >= 25)] = 2\n",
    "c9[np.logical_and(c9 < 35, c9 >= 30)] = 3\n",
    "c9[np.logical_and(c9 < 40, c9 >= 35)] = 4\n",
    "c9[ c9 >= 40 ] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOskTlhci3PM"
   },
   "outputs": [],
   "source": [
    "x[:,9] = c9[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtB6UHEEi3PN",
    "outputId": "9668e1b8-047b-421c-e2fe-43cf015c36cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 5., 5., 1., 2., 3., 3., 1., 2., 5., 1., 2., 2., 2., 2., 2., 2.,\n",
       "       3., 1., 3., 1., 5., 5., 3., 0., 1., 3., 0., 0., 2., 3., 1., 3., 3.,\n",
       "       0., 3., 1., 5., 1., 2., 1., 3., 3., 2., 4., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 1., 3., 5., 3., 4., 2., 2., 2., 5., 1., 1., 1., 1., 1., 1., 3.,\n",
       "       3., 2., 5., 1., 1., 1., 2., 5., 2., 3., 2., 3., 3., 1., 3., 4., 4.,\n",
       "       2., 1., 2., 2., 0., 3., 2., 2., 1., 2., 3., 2., 4., 2., 2., 2., 1.,\n",
       "       2., 2., 0., 1., 3., 4., 1., 2., 1., 3., 4., 1., 0., 2., 1., 2., 3.,\n",
       "       1., 5., 1., 5., 4., 1., 3., 1., 2., 5., 1., 1., 3., 5., 3., 4., 1.,\n",
       "       1., 2., 1., 1., 0., 5., 2., 1., 2., 2., 1., 3., 4., 2., 1., 2., 4.,\n",
       "       4., 5., 1., 2., 5., 1., 1., 1., 3., 1., 1., 1., 5., 3., 3., 1., 3.,\n",
       "       3., 3., 2., 2., 3., 3., 3., 2., 4., 3., 3., 5., 4., 3., 2., 0., 3.,\n",
       "       2., 1., 1., 4., 3., 2., 1., 3., 3., 2., 0., 1., 1., 2., 2., 1., 0.,\n",
       "       3., 4., 3., 2., 1., 0., 1., 2., 1., 1., 3., 2., 2., 1., 2., 0., 3.,\n",
       "       2., 3., 2., 3., 1., 3., 3., 2., 1., 4., 2., 4., 2., 1., 1., 3., 2.,\n",
       "       1., 1., 2., 5., 1., 2., 1., 3., 0., 2., 5., 1., 5., 4., 5., 5., 3.,\n",
       "       2., 2., 2., 1., 2., 5., 2., 4., 2., 0., 2., 0., 5., 0., 5., 1., 2.,\n",
       "       2., 1., 1., 4., 1., 1., 1., 2., 3., 1., 2., 3., 5., 2., 2., 1., 0.,\n",
       "       4., 1., 3., 3., 0., 5., 2., 1., 1., 1., 5., 2., 1., 2., 2., 1., 3.,\n",
       "       1., 1., 2., 2., 2., 1., 1., 1., 1., 2., 3., 5., 2., 2., 3., 5., 1.,\n",
       "       2., 1., 1., 3., 2., 4., 3., 3., 3., 2., 3., 2., 3., 3., 3., 3., 2.,\n",
       "       4., 5., 1., 4., 1., 3., 1., 5., 3., 5., 2., 1., 2., 4., 3., 3., 3.,\n",
       "       2., 1., 2., 5., 1., 2., 2., 1., 4., 3., 0., 0., 1., 2., 4., 5., 5.,\n",
       "       4., 2., 4., 2., 3., 2., 1., 2., 2., 2., 1., 1., 3., 2., 3., 4., 4.,\n",
       "       5., 1., 4., 1., 3., 1., 5., 3., 5., 2., 1., 2., 4., 3., 3., 3., 2.,\n",
       "       1., 2., 5., 1., 2., 2., 1., 4., 3., 0., 0., 1., 2., 4., 5., 5., 4.,\n",
       "       2., 4., 2., 3., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 2., 5., 1.,\n",
       "       1., 1., 2., 5., 2., 3., 2., 3., 3., 1., 3., 4., 4., 2., 1., 2., 2.,\n",
       "       2., 3., 2., 2., 1., 2., 3., 2., 4., 2., 2., 2., 2., 2., 2., 0., 1.,\n",
       "       3., 1., 2., 2., 5., 5., 3., 0., 1., 3., 0., 0., 2., 3., 1., 3., 3.,\n",
       "       0., 3., 1., 5., 1., 2., 1.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFG5xUE2SZvQ"
   },
   "source": [
    "# imputing column 11 (Systolic BP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc_NBW2xSZvR"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 2\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "\n",
    "● Systolic blood pressure below 130 mmHg  == [0]\n",
    "\n",
    "● Systolic blood pressure above 130 mmHg == [1]\n",
    "\n",
    "\n",
    "outlires : \n",
    "[730.0, 1140.0] \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ_4KYVSSZvS"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCbPFUYXSZvS"
   },
   "outputs": [],
   "source": [
    "c10 = np.reshape(x[:,10], (-1, 1))\n",
    "c10[c10 == 730.0] = np.nan\n",
    "c10[c10 == 1140.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB8wCDWRSZvT"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATcnCnW4SZvU"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c10 = imputer.fit_transform(c10)\n",
    "x[:,10] = c10[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXCchbNQSZvU"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Plen__vSZvU"
   },
   "outputs": [],
   "source": [
    "c10[c10 <= 130 ] = 0\n",
    "c10[ c10 > 130 ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58okGJbSSZvV"
   },
   "outputs": [],
   "source": [
    "x[:,10] = c10[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GGlzmYtSZvV",
    "outputId": "8d022eb1-6810-4539-bbb2-f781d291d975"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hut0MdxBWAxd"
   },
   "source": [
    "# imputing column 12 (Diastolic BP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FspdZrZ0WAxd"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 2\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "\n",
    "● Diastolic blood pressure below 85 mmHg  == [0]\n",
    "\n",
    "● Diastolic blood pressure above 85 mmHg == [1]\n",
    "\n",
    "\n",
    "outlires : x\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqo2Mja5WAxd"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1UV08K5WAxe"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pxQjrSQWAxe"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "c11 = np.reshape(x[:,11], (-1, 1))\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c11 = imputer.fit_transform(c11)\n",
    "x[:,11] = c11[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uBZJLCnWAxe"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOy0fAukWAxe"
   },
   "outputs": [],
   "source": [
    "c11[c11 <= 85 ] = 0\n",
    "c11[c11 > 85 ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ1EXQNBWAxe"
   },
   "outputs": [],
   "source": [
    "x[:,11] = c11[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwwJfL4MWAxe",
    "outputId": "cf3b2952-d339-4103-a5fa-22ae2ff781a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2t8lvdzbxUj"
   },
   "source": [
    "# imputing column 13 (waist (cm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDVzLBtVbxUk"
   },
   "source": [
    "zero values : 2\n",
    "\n",
    "null values : 0\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification :\n",
    "\n",
    "● 81 cm or below == [0]\n",
    "\n",
    "● 81 cm - 88 cm == [1]\n",
    "\n",
    "● 88 cm - 94 cm == [2]\n",
    "\n",
    "● 94 cm or above == [3]\n",
    "\n",
    "\n",
    "outlires : [0,0]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cxi2C10XbxUk"
   },
   "source": [
    "# outliers handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qlp4eJad29X"
   },
   "outputs": [],
   "source": [
    "c12 = np.reshape(x[:,12], (-1, 1))\n",
    "c12[c12 == 0.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp0JK0rNbxUl"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4icyB3lAbxUl",
    "outputId": "091541d5-296c-450b-cbd2-8f4b238f6724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81.3       ,  83.8       , 101.6       ,  78.7       ,\n",
       "        81.3       ,  94.        ,  78.7       ,  73.7       ,\n",
       "        78.7       ,  86.4       ,  94.        , 124.5       ,\n",
       "        99.1       ,  71.1       ,  78.7       ,  99.1       ,\n",
       "        86.4       , 101.6       ,  71.1       ,  88.9       ,\n",
       "       116.8       ,  71.1       ,  88.9       , 111.8       ,\n",
       "       127.        , 109.2       ,  86.4       , 109.2       ,\n",
       "        81.3       , 111.8       ,  83.8       ,  81.3       ,\n",
       "       119.4       , 101.6       ,  88.9       ,  96.5       ,\n",
       "        76.2       ,  81.3       , 104.1       , 121.9       ,\n",
       "        83.8       ,  71.1       ,  81.3       ,  88.9       ,\n",
       "        99.1       ,  86.4       , 121.9       ,  88.9       ,\n",
       "        83.8       , 101.6       ,  99.1       , 101.6       ,\n",
       "        73.7       ,  88.9       , 106.7       ,  71.1       ,\n",
       "        96.5       , 104.1       ,  96.5       ,  66.        ,\n",
       "        76.2       ,  83.8       , 116.8       ,  96.5       ,\n",
       "        83.8       ,  81.3       ,  83.8       , 119.4       ,\n",
       "        78.7       ,  78.7       ,  83.8       , 116.8       ,\n",
       "        88.9       , 109.2       , 104.1       ,  91.4       ,\n",
       "        73.7       , 119.4       ,  76.2       ,  83.8       ,\n",
       "       104.1       , 101.6       , 134.6       ,  83.8       ,\n",
       "        94.        , 114.3       ,  99.1       ,  94.        ,\n",
       "       106.7       , 132.1       ,  73.7       ,  88.9       ,\n",
       "        83.8       ,  78.7       , 101.6       ,  91.4       ,\n",
       "       104.1       ,  86.4       ,  86.4       ,  94.        ,\n",
       "        78.7       ,  81.3       ,  91.4       ,  81.3       ,\n",
       "        99.1       , 104.1       , 101.6       ,  99.1       ,\n",
       "        91.4       , 119.4       ,  81.3       ,  83.8       ,\n",
       "        78.7       ,  86.4       ,  94.        ,  76.2       ,\n",
       "       129.5       ,  94.        ,  86.4       ,  91.4       ,\n",
       "        81.3       ,  86.4       , 124.5       , 121.9       ,\n",
       "        83.8       , 106.7       , 109.2       ,  94.        ,\n",
       "       106.7       ,  78.7       ,  91.4       ,  99.1       ,\n",
       "       104.1       ,  78.7       , 106.7       ,  86.4       ,\n",
       "        96.5       , 124.5       ,  86.4       , 121.9       ,\n",
       "       116.8       ,  83.8       ,  66.        ,  94.        ,\n",
       "        96.5       ,  94.        ,  94.        ,  91.4       ,\n",
       "        94.        , 101.6       , 104.1       ,  94.        ,\n",
       "       127.        , 109.2       , 114.3       ,  96.5       ,\n",
       "       101.6       , 101.6       ,  96.5       ,  99.1       ,\n",
       "        86.4       ,  88.9       ,  71.1       ,  78.7       ,\n",
       "        81.3       , 104.1       ,  73.7       ,  73.7       ,\n",
       "        96.5       ,  99.1       ,  99.1       ,  83.8       ,\n",
       "        94.        , 119.4       ,  94.        ,  91.4       ,\n",
       "        83.8       ,  94.        , 121.9       , 111.8       ,\n",
       "       104.1       ,  81.3       ,  78.7       ,  94.        ,\n",
       "       134.6       ,  94.        ,  91.4       , 101.6       ,\n",
       "       101.6       ,  83.8       ,  91.4       , 101.6       ,\n",
       "        91.4       ,  96.5       , 116.8       ,  76.2       ,\n",
       "        99.1       ,  81.3       ,  99.1       ,  86.4       ,\n",
       "        83.8       , 101.6       , 104.1       , 116.8       ,\n",
       "        73.7       ,  96.5       ,  94.        ,  99.1       ,\n",
       "       114.3       ,  76.2       ,  88.9       , 116.8       ,\n",
       "        88.9       , 106.7       ,  88.9       ,  68.6       ,\n",
       "        91.4       ,  81.3       ,  81.3       ,  81.3       ,\n",
       "        96.5       , 104.1       , 101.6       , 124.5       ,\n",
       "       109.2       ,  91.4       ,  94.        ,  88.9       ,\n",
       "       106.7       ,  86.4       ,  86.4       , 129.5       ,\n",
       "       101.6       , 142.2       ,  94.        ,  83.8       ,\n",
       "        81.3       ,  71.1       , 109.2       ,  96.5       ,\n",
       "       132.1       , 106.7       , 109.2       , 101.6       ,\n",
       "        83.8       ,  76.2       ,  73.7       ,  91.4       ,\n",
       "       106.7       ,  78.7       ,  86.4       ,  99.1       ,\n",
       "        86.4       ,  96.5       ,  94.        , 109.2       ,\n",
       "        88.9       ,  91.4       ,  76.2       ,  83.8       ,\n",
       "        94.        , 106.7       ,  76.2       , 109.2       ,\n",
       "        81.3       , 109.2       , 114.3       , 109.2       ,\n",
       "        96.5       ,  91.4       ,  86.4       , 104.1       ,\n",
       "        96.5       ,  78.7       ,  78.7       ,  91.4       ,\n",
       "       124.5       ,  96.5       , 116.8       ,  96.5       ,\n",
       "        96.5       ,  76.2       ,  96.5       ,  86.4       ,\n",
       "        99.1       ,  81.3       ,  96.5       ,  96.5       ,\n",
       "        94.        ,  91.4       ,  91.4       ,  78.7       ,\n",
       "        99.1       , 101.6       ,  83.8       ,  81.3       ,\n",
       "        96.5       , 121.9       , 106.7       ,  91.4       ,\n",
       "       101.6       , 101.6       , 104.1       , 124.5       ,\n",
       "        83.8       , 101.6       ,  99.1       ,  96.5       ,\n",
       "       127.        ,  88.9       , 139.7       , 116.8       ,\n",
       "        88.9       ,  94.        ,  86.4       ,  94.        ,\n",
       "       119.4       , 111.8       , 114.3       ,  99.1       ,\n",
       "       111.8       ,  86.4       , 114.3       , 111.8       ,\n",
       "       111.8       , 111.8       , 111.8       ,  83.8       ,\n",
       "       101.6       ,  94.        , 109.2       , 106.7       ,\n",
       "       109.2       , 106.7       ,  78.7       , 101.6       ,\n",
       "       121.9       ,  78.7       , 114.3       , 114.3       ,\n",
       "        91.4       ,  83.8       ,  94.        ,  91.4       ,\n",
       "        91.4       ,  81.3       ,  94.        ,  73.7       ,\n",
       "        73.7       , 114.3       , 104.1       , 106.7       ,\n",
       "       114.3       , 129.5       ,  83.8       ,  96.5       ,\n",
       "       121.9       ,  86.4       , 114.3       ,  99.1       ,\n",
       "       101.6       ,  99.1       , 109.2       ,  96.5       ,\n",
       "        99.1       , 104.1       , 106.7       , 111.8       ,\n",
       "       111.8       , 101.6       ,  94.        ,  78.7       ,\n",
       "        73.7       ,  88.9       ,  96.21285141,  96.5       ,\n",
       "        91.4       , 101.6       ,  86.4       ,  99.1       ,\n",
       "       119.4       , 111.8       ,  91.4       , 104.1       ,\n",
       "        94.        ,  88.9       , 104.1       ,  78.7       ,\n",
       "        88.9       , 129.5       ,  91.4       ,  83.8       ,\n",
       "        94.        ,  91.4       ,  91.4       ,  81.3       ,\n",
       "        94.        ,  73.7       ,  73.7       , 114.3       ,\n",
       "       104.1       , 106.7       , 114.3       , 129.5       ,\n",
       "        83.8       ,  96.5       , 121.9       ,  86.4       ,\n",
       "       114.3       ,  99.1       , 101.6       ,  99.1       ,\n",
       "       109.2       ,  96.5       ,  99.1       , 104.1       ,\n",
       "       106.7       , 111.8       , 111.8       , 101.6       ,\n",
       "        94.        ,  78.7       ,  73.7       ,  88.9       ,\n",
       "        83.8       ,  96.5       ,  91.4       , 101.6       ,\n",
       "        86.4       ,  99.1       , 119.4       ,  83.8       ,\n",
       "       116.8       ,  96.5       ,  83.8       ,  81.3       ,\n",
       "        83.8       , 119.4       ,  78.7       ,  78.7       ,\n",
       "        83.8       , 116.8       ,  88.9       , 109.2       ,\n",
       "       104.1       ,  91.4       ,  73.7       , 119.4       ,\n",
       "        76.2       ,  83.8       , 104.1       , 101.6       ,\n",
       "       134.6       ,  83.8       ,  94.        , 114.3       ,\n",
       "        99.1       ,  94.        , 106.7       , 132.1       ,\n",
       "        73.7       ,  96.21285141,  83.8       ,  78.7       ,\n",
       "       101.6       ,  91.4       , 104.1       ,  86.4       ,\n",
       "        86.4       ,  94.        ,  78.7       ,  81.3       ,\n",
       "        91.4       ,  81.3       ,  99.1       , 104.1       ,\n",
       "       101.6       ,  71.1       ,  88.9       , 116.8       ,\n",
       "        71.1       ,  88.9       , 111.8       , 127.        ,\n",
       "       109.2       ,  86.4       , 109.2       ,  81.3       ,\n",
       "       111.8       ,  83.8       ,  81.3       , 119.4       ,\n",
       "       101.6       ,  88.9       ,  96.5       ,  76.2       ,\n",
       "        81.3       , 104.1       , 121.9       ,  83.8       ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import copy\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "c12 = imputer.fit_transform(c12)\n",
    "c12_not_categ = copy.deepcopy(c12[:,0]) # not categorized data is needed later \n",
    "c12_not_categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm38G-9KbxUm"
   },
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rlfCgW_bxUm"
   },
   "outputs": [],
   "source": [
    "c12[c12 < 81 ] = 0\n",
    "c12[np.logical_and(c12 < 88, c12 >= 81)] = 1\n",
    "c12[np.logical_and(c12 < 94, c12 >= 88)] = 2\n",
    "c12[ c12 >= 94 ] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBW2kaEXbxUm"
   },
   "outputs": [],
   "source": [
    "x[:,12] = c12[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLWsvNrubxUn",
    "outputId": "cdfe4cab-43a0-4737-f334-f70e355d2464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 3., 0., 1., 3., 0., 0., 0., 1., 3., 3., 3., 0., 0., 3., 1.,\n",
       "       3., 0., 2., 3., 0., 2., 3., 3., 3., 1., 3., 1., 3., 1., 1., 3., 3.,\n",
       "       2., 3., 0., 1., 3., 3., 1., 0., 1., 2., 3., 1., 3., 2., 1., 3., 3.,\n",
       "       3., 0., 2., 3., 0., 3., 3., 3., 0., 0., 1., 3., 3., 1., 1., 1., 3.,\n",
       "       0., 0., 1., 3., 2., 3., 3., 2., 0., 3., 0., 1., 3., 3., 3., 1., 3.,\n",
       "       3., 3., 3., 3., 3., 0., 2., 1., 0., 3., 2., 3., 1., 1., 3., 0., 1.,\n",
       "       2., 1., 3., 3., 3., 3., 2., 3., 1., 1., 0., 1., 3., 0., 3., 3., 1.,\n",
       "       2., 1., 1., 3., 3., 1., 3., 3., 3., 3., 0., 2., 3., 3., 0., 3., 1.,\n",
       "       3., 3., 1., 3., 3., 1., 0., 3., 3., 3., 3., 2., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 1., 2., 0., 0., 1., 3., 0., 0., 3., 3.,\n",
       "       3., 1., 3., 3., 3., 2., 1., 3., 3., 3., 3., 1., 0., 3., 3., 3., 2.,\n",
       "       3., 3., 1., 2., 3., 2., 3., 3., 0., 3., 1., 3., 1., 1., 3., 3., 3.,\n",
       "       0., 3., 3., 3., 3., 0., 2., 3., 2., 3., 2., 0., 2., 1., 1., 1., 3.,\n",
       "       3., 3., 3., 3., 2., 3., 2., 3., 1., 1., 3., 3., 3., 3., 1., 1., 0.,\n",
       "       3., 3., 3., 3., 3., 3., 1., 0., 0., 2., 3., 0., 1., 3., 1., 3., 3.,\n",
       "       3., 2., 2., 0., 1., 3., 3., 0., 3., 1., 3., 3., 3., 3., 2., 1., 3.,\n",
       "       3., 0., 0., 2., 3., 3., 3., 3., 3., 0., 3., 1., 3., 1., 3., 3., 3.,\n",
       "       2., 2., 0., 3., 3., 1., 1., 3., 3., 3., 2., 3., 3., 3., 3., 1., 3.,\n",
       "       3., 3., 3., 2., 3., 3., 2., 3., 1., 3., 3., 3., 3., 3., 3., 1., 3.,\n",
       "       3., 3., 3., 3., 1., 3., 3., 3., 3., 3., 3., 0., 3., 3., 0., 3., 3.,\n",
       "       2., 1., 3., 2., 2., 1., 3., 0., 0., 3., 3., 3., 3., 3., 1., 3., 3.,\n",
       "       1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 2.,\n",
       "       3., 3., 2., 3., 1., 3., 3., 3., 2., 3., 3., 2., 3., 0., 2., 3., 2.,\n",
       "       1., 3., 2., 2., 1., 3., 0., 0., 3., 3., 3., 3., 3., 1., 3., 3., 1.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 2., 1.,\n",
       "       3., 2., 3., 1., 3., 3., 1., 3., 3., 1., 1., 1., 3., 0., 0., 1., 3.,\n",
       "       2., 3., 3., 2., 0., 3., 0., 1., 3., 3., 3., 1., 3., 3., 3., 3., 3.,\n",
       "       3., 0., 3., 1., 0., 3., 2., 3., 1., 1., 3., 0., 1., 2., 1., 3., 3.,\n",
       "       3., 0., 2., 3., 0., 2., 3., 3., 3., 1., 3., 1., 3., 1., 1., 3., 3.,\n",
       "       2., 3., 0., 1., 3., 3., 1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o36TCRjbevO1"
   },
   "source": [
    "# imputing column 14 (hip (cm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxZ0_lgTevO2"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 0\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification : No category\n",
    "\n",
    "outlires : x\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJeJZJnVevO6",
    "outputId": "d009f301-048a-4af5-f2f9-9a1eaa46e8b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 96.5, 101.6, 114.3,  99.1,  99.1, 101.6,  94. ,  91.4,  99.1,\n",
       "       116.8, 104.1, 147.3, 109.2,  99.1,  99.1, 109.2,  99.1, 132.1,\n",
       "        88.9,  96.5, 127. ,  88.9, 101.6, 127. , 124.5, 119.4, 104.1,\n",
       "       114.3, 101.6, 119.4,  96.5,  86.4, 147.3, 106.7,  99.1, 116.8,\n",
       "        88.9, 101.6, 111.8, 124.5,  99.1,  94. ,  88.9, 104.1, 119.4,\n",
       "       109.2, 129.5, 111.8,  96.5, 106.7, 109.2, 104.1,  88.9,  96.5,\n",
       "       116.8,  81.3, 114.3, 132.1,  99.1,  94. ,  91.4,  96.5, 121.9,\n",
       "       106.7, 101.6,  86.4, 101.6, 139.7,  96.5,  99.1, 106.7, 124.5,\n",
       "       104.1, 129.5, 119.4, 106.7, 101.6, 134.6, 111.8,  96.5, 106.7,\n",
       "       114.3, 142.2, 101.6, 119.4, 147.3, 111.8, 104.1, 116.8, 147.3,\n",
       "        88.9,  96.5, 116.8,  99.1, 119.4, 106.7, 106.7, 116.8, 109.2,\n",
       "       109.2,  88.9,  94. , 111.8,  96.5, 104.1, 114.3, 106.7, 114.3,\n",
       "       101.6, 132.1,  94. ,  91.4,  96.5, 101.6, 104.1,  86.4, 124.5,\n",
       "       106.7,  99.1, 109.2, 106.7, 106.7, 144.8, 129.5,  99.1, 121.9,\n",
       "       119.4, 114.3, 129.5,  88.9, 119.4, 104.1, 129.5,  99.1, 127. ,\n",
       "       101.6, 116.8, 147.3,  94. , 139.7, 137.2, 106.7,  83.8, 109.2,\n",
       "       124.5,  96.5,  99.1, 111.8, 109.2, 111.8,  99.1, 109.2, 152.4,\n",
       "       137.2, 124.5, 104.1, 111.8, 119.4, 101.6, 111.8,  99.1, 116.8,\n",
       "        91.4, 101.6, 101.6, 121.9,  99.1,  76.2, 104.1, 104.1, 104.1,\n",
       "       114.3, 116.8, 132.1, 101.6, 106.7, 101.6, 104.1, 134.6, 119.4,\n",
       "       111.8, 104.1,  96.5, 101.6, 157.5, 111.8, 116.8, 114.3, 124.5,\n",
       "        94. ,  99.1, 109.2, 109.2, 111.8, 124.5,  86.4, 111.8,  96.5,\n",
       "       111.8, 101.6,  96.5, 109.2, 114.3, 124.5,  96.5, 116.8, 114.3,\n",
       "       111.8, 116.8,  91.4,  99.1, 137.2,  99.1, 119.4, 106.7,  83.8,\n",
       "       101.6,  88.9,  96.5, 104.1, 106.7, 106.7, 124.5, 114.3, 119.4,\n",
       "       101.6, 109.2,  94. , 137.2,  99.1, 104.1, 162.6, 114.3, 124.5,\n",
       "       104.1,  91.4, 109.2,  94. , 116.8, 106.7, 149.9, 134.6, 121.9,\n",
       "       109.2,  99.1,  83.8,  88.9, 101.6, 124.5,  88.9, 106.7, 119.4,\n",
       "       109.2, 116.8, 101.6, 114.3,  99.1, 104.1,  94. ,  99.1, 109.2,\n",
       "       106.7,  94. , 119.4,  83.8, 132.1, 127. , 121.9,  96.5,  99.1,\n",
       "        96.5, 127. ,  99.1,  94. ,  91.4,  99.1, 144.8, 109.2, 111.8,\n",
       "       104.1, 109.2,  94. ,  94. , 111.8, 114.3,  96.5, 114.3, 109.2,\n",
       "       101.6,  96.5, 101.6, 101.6, 114.3, 106.7,  99.1,  86.4, 111.8,\n",
       "       127. , 109.2, 104.1, 127. , 111.8, 119.4, 109.2,  96.5, 106.7,\n",
       "       104.1, 104.1, 119.4,  99.1, 157.5, 129.5,  99.1, 106.7,  99.1,\n",
       "       101.6, 114.3, 134.6, 137.2,  96.5, 121.9, 106.7, 116.8, 119.4,\n",
       "       119.4, 119.4, 104.1, 104.1, 134.6, 106.7, 119.4,  99.1, 124.5,\n",
       "       109.2,  83.8, 104.1, 111.8,  96.5, 121.9, 116.8, 121.9,  96.5,\n",
       "       104.1, 109.2,  99.1,  96.5, 109.2,  83.8,  94. , 121.9, 119.4,\n",
       "       104.1, 121.9, 137.2, 104.1, 101.6, 129.5, 106.7, 116.8, 119.4,\n",
       "       101.6, 101.6, 116.8, 111.8, 104.1, 116.8, 121.9, 106.7, 114.3,\n",
       "       104.1, 104.1,  83.8,  83.8, 101.6,  86.4,  99.1, 114.3, 101.6,\n",
       "        96.5, 104.1, 121.9, 104.1, 109.2, 116.8, 109.2, 101.6, 121.9,\n",
       "       104.1,  96.5, 129.5, 121.9,  96.5, 104.1, 109.2,  99.1,  96.5,\n",
       "       109.2,  83.8,  94. , 121.9, 119.4, 104.1, 121.9, 137.2, 104.1,\n",
       "       101.6, 129.5, 106.7, 116.8, 119.4, 101.6, 101.6, 116.8, 111.8,\n",
       "       104.1, 116.8, 121.9, 106.7, 114.3, 104.1, 104.1,  83.8,  83.8,\n",
       "       101.6,  86.4,  99.1, 114.3, 101.6,  96.5, 104.1, 121.9,  96.5,\n",
       "       121.9, 106.7, 101.6,  86.4, 101.6, 139.7,  96.5,  99.1, 106.7,\n",
       "       124.5, 104.1, 129.5, 119.4, 106.7, 101.6, 134.6, 111.8,  96.5,\n",
       "       106.7, 114.3, 142.2, 101.6, 119.4, 147.3, 111.8, 104.1, 116.8,\n",
       "       147.3,  88.9,  96.5, 116.8,  99.1, 119.4, 106.7, 106.7, 116.8,\n",
       "       109.2, 109.2,  88.9,  94. , 111.8,  96.5, 104.1, 114.3, 106.7,\n",
       "        88.9,  96.5, 127. ,  88.9, 101.6, 127. , 124.5, 119.4, 104.1,\n",
       "       114.3, 101.6, 119.4,  96.5,  86.4, 147.3, 106.7,  99.1, 116.8,\n",
       "        88.9, 101.6, 111.8, 124.5,  99.1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8nZvRuwodKt"
   },
   "source": [
    "# New Features\n",
    "Following the recommendation of the medical team, 2 features of WHR (Waist/Hip) and WSR (Waist / Stature) are added to the data. Women with high WHRs in the Presence of this risk factor are notably at risk for diabetes [1]. Also, BMI and WSR have similar effects on accruing diabetes [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oguPwRzKot40"
   },
   "source": [
    "## WHR\n",
    "waist-to-hip ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LD72ptqXoias",
    "outputId": "b3e78ebe-3e2a-489b-c12b-72f8acf2c8c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248705, 0.82480315, 0.88888889, 0.79414733, 0.82038345,\n",
       "       0.92519685, 0.83723404, 0.80634573, 0.79414733, 0.73972603,\n",
       "       0.90297791, 0.84521385, 0.90750916, 0.71745711, 0.79414733,\n",
       "       0.90750916, 0.87184662, 0.76911431, 0.79977503, 0.92124352,\n",
       "       0.91968504, 0.79977503, 0.875     , 0.88031496, 1.02008032,\n",
       "       0.91457286, 0.82997118, 0.95538058, 0.80019685, 0.93634841,\n",
       "       0.86839378, 0.94097222, 0.81059063, 0.95220244, 0.89707366,\n",
       "       0.82619863, 0.85714286, 0.80019685, 0.93112701, 0.97911647,\n",
       "       0.84561049, 0.75638298, 0.91451069, 0.85398655, 0.82998325,\n",
       "       0.79120879, 0.94131274, 0.79516995, 0.86839378, 0.95220244,\n",
       "       0.90750916, 0.97598463, 0.82902137, 0.92124352, 0.9135274 ,\n",
       "       0.87453875, 0.84426947, 0.78803936, 0.97376387, 0.70212766,\n",
       "       0.83369803, 0.86839378, 0.95816243, 0.90440487, 0.82480315,\n",
       "       0.94097222, 0.82480315, 0.85468862, 0.81554404, 0.79414733,\n",
       "       0.78537957, 0.93815261, 0.85398655, 0.84324324, 0.8718593 ,\n",
       "       0.85660731, 0.7253937 , 0.88707281, 0.68157424, 0.86839378,\n",
       "       0.97563261, 0.88888889, 0.94655415, 0.82480315, 0.78726968,\n",
       "       0.77596741, 0.88640429, 0.90297791, 0.9135274 , 0.89680923,\n",
       "       0.82902137, 0.92124352, 0.71746575, 0.79414733, 0.85092127,\n",
       "       0.85660731, 0.97563261, 0.73972603, 0.79120879, 0.86080586,\n",
       "       0.88526434, 0.86489362, 0.81753131, 0.84248705, 0.95196926,\n",
       "       0.91076115, 0.95220244, 0.86701662, 0.8996063 , 0.90386071,\n",
       "       0.86489362, 0.91684902, 0.81554404, 0.8503937 , 0.90297791,\n",
       "       0.88194444, 1.04016064, 0.8809747 , 0.87184662, 0.83699634,\n",
       "       0.76194939, 0.80974695, 0.85980663, 0.94131274, 0.84561049,\n",
       "       0.87530763, 0.91457286, 0.8223972 , 0.82393822, 0.88526434,\n",
       "       0.76549414, 0.95196926, 0.803861  , 0.79414733, 0.84015748,\n",
       "       0.8503937 , 0.82619863, 0.84521385, 0.91914894, 0.87258411,\n",
       "       0.85131195, 0.78537957, 0.7875895 , 0.86080586, 0.7751004 ,\n",
       "       0.97409326, 0.94853683, 0.81753131, 0.86080586, 0.90876565,\n",
       "       1.05045409, 0.86080586, 0.83333333, 0.79591837, 0.91807229,\n",
       "       0.92699328, 0.90876565, 0.85092127, 0.94980315, 0.88640429,\n",
       "       0.87184662, 0.76113014, 0.77789934, 0.7746063 , 0.80019685,\n",
       "       0.85397867, 0.74369324, 0.9671916 , 0.92699328, 0.95196926,\n",
       "       0.95196926, 0.73315836, 0.80479452, 0.90386071, 0.92519685,\n",
       "       0.85660731, 0.82480315, 0.90297791, 0.90564636, 0.93634841,\n",
       "       0.93112701, 0.78097983, 0.81554404, 0.92519685, 0.85460317,\n",
       "       0.84078712, 0.78253425, 0.88888889, 0.81606426, 0.89148936,\n",
       "       0.92230071, 0.93040293, 0.83699634, 0.86314848, 0.93815261,\n",
       "       0.88194444, 0.88640429, 0.84248705, 0.88640429, 0.8503937 ,\n",
       "       0.86839378, 0.93040293, 0.91076115, 0.93815261, 0.76373057,\n",
       "       0.82619863, 0.8223972 , 0.88640429, 0.97859589, 0.83369803,\n",
       "       0.89707366, 0.85131195, 0.89707366, 0.89363484, 0.83317713,\n",
       "       0.81861575, 0.8996063 , 0.91451069, 0.84248705, 0.78097983,\n",
       "       0.90440487, 0.97563261, 0.81606426, 1.08923885, 0.91457286,\n",
       "       0.8996063 , 0.86080586, 0.94574468, 0.77769679, 0.87184662,\n",
       "       0.82997118, 0.79643296, 0.88888889, 1.14216867, 0.90297791,\n",
       "       0.91684902, 0.74450549, 0.75638298, 0.93493151, 0.90440487,\n",
       "       0.88125417, 0.79271917, 0.89581624, 0.93040293, 0.84561049,\n",
       "       0.90930788, 0.82902137, 0.8996063 , 0.85702811, 0.88526434,\n",
       "       0.80974695, 0.82998325, 0.79120879, 0.82619863, 0.92519685,\n",
       "       0.95538058, 0.89707366, 0.87800192, 0.8106383 , 0.84561049,\n",
       "       0.86080586, 1.        , 0.8106383 , 0.91457286, 0.97016706,\n",
       "       0.82664648, 0.9       , 0.89581624, 1.        , 0.92230071,\n",
       "       0.89533679, 0.81968504, 0.97376387, 0.83723404, 0.86105033,\n",
       "       0.92230071, 0.85980663, 0.88369963, 1.04472272, 0.92699328,\n",
       "       0.88369963, 0.8106383 , 1.02659574, 0.77280859, 0.86701662,\n",
       "       0.84248705, 0.84426947, 0.88369963, 0.92519685, 0.94715026,\n",
       "       0.8996063 , 0.7746063 , 0.86701662, 0.95220244, 0.84561049,\n",
       "       0.94097222, 0.86314848, 0.95984252, 0.97710623, 0.87800192,\n",
       "       0.8       , 0.90876565, 0.8718593 , 1.14010989, 0.86839378,\n",
       "       0.95220244, 0.95196926, 0.92699328, 1.06365159, 0.89707366,\n",
       "       0.88698413, 0.9019305 , 0.89707366, 0.8809747 , 0.87184662,\n",
       "       0.92519685, 1.04461942, 0.83060921, 0.83309038, 1.02694301,\n",
       "       0.9171452 , 0.80974695, 0.97859589, 0.93634841, 0.93634841,\n",
       "       0.93634841, 1.07396734, 0.8049952 , 0.75482912, 0.8809747 ,\n",
       "       0.91457286, 1.07669021, 0.87710843, 0.97710623, 0.93914081,\n",
       "       0.97598463, 1.09033989, 0.81554404, 0.93765381, 0.97859589,\n",
       "       0.74979491, 0.86839378, 0.90297791, 0.83699634, 0.92230071,\n",
       "       0.84248705, 0.86080586, 0.87947494, 0.78404255, 0.93765381,\n",
       "       0.8718593 , 1.02497598, 0.93765381, 0.94387755, 0.8049952 ,\n",
       "       0.94980315, 0.94131274, 0.80974695, 0.97859589, 0.82998325,\n",
       "       1.        , 0.9753937 , 0.93493151, 0.86314848, 0.95196926,\n",
       "       0.89126712, 0.87530763, 1.04779756, 0.97812773, 0.97598463,\n",
       "       0.90297791, 0.93914081, 0.87947494, 0.875     , 1.11357467,\n",
       "       0.97376387, 0.79965004, 1.        , 0.89533679, 0.95196926,\n",
       "       0.97949139, 1.07396734, 0.83699634, 0.89126712, 0.86080586,\n",
       "       0.875     , 0.85397867, 0.75600384, 0.92124352, 1.        ,\n",
       "       0.74979491, 0.86839378, 0.90297791, 0.83699634, 0.92230071,\n",
       "       0.84248705, 0.86080586, 0.87947494, 0.78404255, 0.93765381,\n",
       "       0.8718593 , 1.02497598, 0.93765381, 0.94387755, 0.8049952 ,\n",
       "       0.94980315, 0.94131274, 0.80974695, 0.97859589, 0.82998325,\n",
       "       1.        , 0.9753937 , 0.93493151, 0.86314848, 0.95196926,\n",
       "       0.89126712, 0.87530763, 1.04779756, 0.97812773, 0.97598463,\n",
       "       0.90297791, 0.93914081, 0.87947494, 0.875     , 0.96990741,\n",
       "       0.97376387, 0.79965004, 1.        , 0.89533679, 0.95196926,\n",
       "       0.97949139, 0.86839378, 0.95816243, 0.90440487, 0.82480315,\n",
       "       0.94097222, 0.82480315, 0.85468862, 0.81554404, 0.79414733,\n",
       "       0.78537957, 0.93815261, 0.85398655, 0.84324324, 0.8718593 ,\n",
       "       0.85660731, 0.7253937 , 0.88707281, 0.68157424, 0.86839378,\n",
       "       0.97563261, 0.88888889, 0.94655415, 0.82480315, 0.78726968,\n",
       "       0.77596741, 0.88640429, 0.90297791, 0.9135274 , 0.89680923,\n",
       "       0.82902137, 0.99702437, 0.71746575, 0.79414733, 0.85092127,\n",
       "       0.85660731, 0.97563261, 0.73972603, 0.79120879, 0.86080586,\n",
       "       0.88526434, 0.86489362, 0.81753131, 0.84248705, 0.95196926,\n",
       "       0.91076115, 0.95220244, 0.79977503, 0.92124352, 0.91968504,\n",
       "       0.79977503, 0.875     , 0.88031496, 1.02008032, 0.91457286,\n",
       "       0.82997118, 0.95538058, 0.80019685, 0.93634841, 0.86839378,\n",
       "       0.94097222, 0.81059063, 0.95220244, 0.89707366, 0.82619863,\n",
       "       0.85714286, 0.80019685, 0.93112701, 0.97911647, 0.84561049])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## imputed not categorized  parameters \n",
    "#waist == c12_not_categ\n",
    "#hip   == x[:,13]\n",
    "whr = np.divide(c12_not_categ , x[:,13])\n",
    "whr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIedPgiztUR9",
    "outputId": "0bec6c28-2b99-43c2-89b2-2e27b70b26d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 2., 0., 0., 3., 1., 0., 0., 0., 2., 1., 2., 0., 0., 2., 2.,\n",
       "       0., 0., 3., 3., 0., 2., 2., 3., 3., 0., 3., 0., 3., 2., 3., 0., 3.,\n",
       "       2., 0., 1., 0., 3., 3., 1., 0., 3., 1., 0., 0., 3., 0., 2., 3., 2.,\n",
       "       3., 0., 3., 3., 2., 1., 0., 3., 0., 1., 2., 3., 2., 0., 3., 0., 1.,\n",
       "       0., 0., 0., 3., 1., 1., 2., 1., 0., 2., 0., 2., 3., 2., 3., 0., 0.,\n",
       "       0., 2., 2., 3., 2., 0., 3., 0., 0., 1., 1., 3., 0., 0., 2., 2., 2.,\n",
       "       0., 1., 3., 3., 3., 2., 2., 2., 2., 3., 0., 1., 2., 2., 3., 2., 2.,\n",
       "       1., 0., 0., 1., 3., 1., 2., 3., 0., 0., 2., 0., 3., 0., 0., 1., 1.,\n",
       "       0., 1., 3., 2., 1., 0., 0., 2., 0., 3., 3., 0., 2., 2., 3., 2., 1.,\n",
       "       0., 3., 3., 2., 1., 3., 2., 2., 0., 0., 0., 0., 1., 0., 3., 3., 3.,\n",
       "       3., 0., 0., 2., 3., 1., 0., 2., 2., 3., 3., 0., 0., 3., 1., 1., 0.,\n",
       "       2., 0., 2., 3., 3., 1., 2., 3., 2., 2., 1., 2., 1., 2., 3., 3., 3.,\n",
       "       0., 0., 0., 2., 3., 1., 2., 1., 2., 2., 1., 0., 2., 3., 1., 0., 2.,\n",
       "       3., 0., 3., 3., 2., 2., 3., 0., 2., 0., 0., 2., 3., 2., 3., 0., 0.,\n",
       "       3., 2., 2., 0., 2., 3., 1., 2., 0., 2., 1., 2., 0., 0., 0., 0., 3.,\n",
       "       3., 2., 2., 0., 1., 2., 3., 0., 3., 3., 0., 2., 2., 3., 3., 2., 0.,\n",
       "       3., 1., 2., 3., 1., 2., 3., 3., 2., 0., 3., 0., 2., 1., 1., 2., 3.,\n",
       "       3., 2., 0., 2., 3., 1., 3., 2., 3., 3., 2., 0., 2., 2., 3., 2., 3.,\n",
       "       3., 3., 3., 2., 2., 2., 2., 2., 2., 3., 3., 1., 1., 3., 3., 0., 3.,\n",
       "       3., 3., 3., 3., 0., 0., 2., 3., 3., 2., 3., 3., 3., 3., 0., 3., 3.,\n",
       "       0., 2., 2., 1., 3., 1., 2., 2., 0., 3., 2., 3., 3., 3., 0., 3., 3.,\n",
       "       0., 3., 0., 3., 3., 3., 2., 3., 2., 2., 3., 3., 3., 2., 3., 2., 2.,\n",
       "       3., 3., 0., 3., 2., 3., 3., 3., 1., 2., 2., 2., 1., 0., 3., 3., 0.,\n",
       "       2., 2., 1., 3., 1., 2., 2., 0., 3., 2., 3., 3., 3., 0., 3., 3., 0.,\n",
       "       3., 0., 3., 3., 3., 2., 3., 2., 2., 3., 3., 3., 2., 3., 2., 2., 3.,\n",
       "       3., 0., 3., 2., 3., 3., 2., 3., 2., 0., 3., 0., 1., 0., 0., 0., 3.,\n",
       "       1., 1., 2., 1., 0., 2., 0., 2., 3., 2., 3., 0., 0., 0., 2., 2., 3.,\n",
       "       2., 0., 3., 0., 0., 1., 1., 3., 0., 0., 2., 2., 2., 0., 1., 3., 3.,\n",
       "       3., 0., 3., 3., 0., 2., 2., 3., 3., 0., 3., 0., 3., 2., 3., 0., 3.,\n",
       "       2., 0., 1., 0., 3., 3., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#categorize whr\n",
    "whr[whr >= 0.91 ] = 3       #Start with x>= 0.91 with rank of 3 \n",
    "whr[np.logical_and(whr < 0.91, whr >= 0.86)] = 2\n",
    "whr[np.logical_and(whr < 0.86, whr >= 0.83)] = 1\n",
    "whr[whr < 0.83 ] = 0\n",
    "whr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is03QnldttON"
   },
   "outputs": [],
   "source": [
    "##concatenate new feature to x\n",
    "y_whr = whr.reshape(len(whr),1)\n",
    "x = np.concatenate((x, y_whr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csApO25Jv0p9",
    "outputId": "5ae1874f-aa89-414a-83a6-31195c83bc86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,   0. ,   1. , ...,   1. ,  96.5,   1. ],\n",
       "       [  0. ,   0. ,   1. , ...,   1. , 101.6,   0. ],\n",
       "       [  1. ,   0. ,   1. , ...,   3. , 114.3,   2. ],\n",
       "       ...,\n",
       "       [  2. ,   0. ,   0. , ...,   3. , 111.8,   3. ],\n",
       "       [  1. ,   0. ,   1. , ...,   3. , 124.5,   3. ],\n",
       "       [  1. ,   0. ,   0. , ...,   1. ,  99.1,   1. ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QobqM0MFwGRD"
   },
   "source": [
    "## WSR\n",
    "waist-to-stature ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "149h6vmnwGRE",
    "outputId": "0e695103-4db6-40e1-851e-34428f3351bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52485474, 0.54986877, 0.59694477, 0.48400984, 0.45725534,\n",
       "       0.57810578, 0.49968254, 0.40295243, 0.46239718, 0.58655804,\n",
       "       0.52868391, 0.7209033 , 0.60024228, 0.444375  , 0.44894467,\n",
       "       0.619375  , 0.47920133, 0.65590704, 0.444375  , 0.54674047,\n",
       "       0.64780921, 0.45142857, 0.53042959, 0.67716535, 0.70438159,\n",
       "       0.70497095, 0.47238928, 0.66141732, 0.42124352, 0.63776383,\n",
       "       0.54099419, 0.48508353, 0.74625   , 0.66666667, 0.49306711,\n",
       "       0.61269841, 0.41100324, 0.53346457, 0.56916348, 0.66648442,\n",
       "       0.47131609, 0.444375  , 0.49242883, 0.58333333, 0.60024228,\n",
       "       0.50763807, 0.69537935, 0.52232667, 0.45817387, 0.635     ,\n",
       "       0.58225617, 0.58830342, 0.42042213, 0.52232667, 0.666875  ,\n",
       "       0.43726937, 0.59348093, 0.60277939, 0.55877244, 0.39975772,\n",
       "       0.48380952, 0.51537515, 0.71832718, 0.55877244, 0.47803765,\n",
       "       0.51619048, 0.52375   , 0.7124105 , 0.43028978, 0.49968254,\n",
       "       0.50757117, 0.66628637, 0.48605796, 0.59704757, 0.62112172,\n",
       "       0.58031746, 0.44639612, 0.70152761, 0.44770858, 0.5       ,\n",
       "       0.61163337, 0.56350527, 0.85460317, 0.50757117, 0.5968254 ,\n",
       "       0.714375  , 0.55736783, 0.56935191, 0.67746032, 0.75356532,\n",
       "       0.43973747, 0.50713063, 0.52375   , 0.43028978, 0.61538462,\n",
       "       0.50693289, 0.56148867, 0.56692913, 0.54      , 0.5213533 ,\n",
       "       0.46957041, 0.43851133, 0.49972663, 0.43244681, 0.5653166 ,\n",
       "       0.57737105, 0.59694477, 0.60947109, 0.5213919 , 0.7124105 ,\n",
       "       0.5       , 0.52375   , 0.44894467, 0.48593926, 0.53622362,\n",
       "       0.46863469, 0.72834646, 0.56085919, 0.57638426, 0.50693289,\n",
       "       0.54236157, 0.53136531, 0.76568266, 0.71621622, 0.5       ,\n",
       "       0.60011249, 0.63231036, 0.57810578, 0.6888315 , 0.5164042 ,\n",
       "       0.5213919 , 0.5653166 , 0.61163337, 0.5164042 , 0.72437203,\n",
       "       0.50028952, 0.603125  , 0.76568266, 0.50028952, 0.69537935,\n",
       "       0.69689737, 0.54986877, 0.41904762, 0.54429647, 0.73050719,\n",
       "       0.5968254 , 0.52868391, 0.58031746, 0.57810578, 0.55549481,\n",
       "       0.57737105, 0.62708472, 0.79375   , 0.63231036, 0.62493166,\n",
       "       0.52049622, 0.61538462, 0.67778519, 0.56698002, 0.62920635,\n",
       "       0.49286937, 0.53846154, 0.38873701, 0.43649473, 0.508125  ,\n",
       "       0.650625  , 0.49166111, 0.47579083, 0.56698002, 0.62920635,\n",
       "       0.59128878, 0.50757117, 0.49343832, 0.70152761, 0.56935191,\n",
       "       0.5900581 , 0.53206349, 0.56085919, 0.7496925 , 0.63776383,\n",
       "       0.61163337, 0.57172996, 0.49968254, 0.57810578, 0.82779828,\n",
       "       0.5213533 , 0.56211562, 0.64507937, 0.61538462, 0.48523451,\n",
       "       0.50693289, 0.62484625, 0.58031746, 0.59348093, 0.68625147,\n",
       "       0.45465394, 0.5653166 , 0.47767333, 0.60947109, 0.49286937,\n",
       "       0.49236193, 0.57957787, 0.61163337, 0.63860033, 0.46793651,\n",
       "       0.603125  , 0.5968254 , 0.59128878, 0.67156287, 0.46863469,\n",
       "       0.51476549, 0.70745003, 0.555625  , 0.57551241, 0.555625  ,\n",
       "       0.39722061, 0.51406074, 0.45725534, 0.508125  , 0.47075854,\n",
       "       0.55877244, 0.61163337, 0.64507937, 0.69051581, 0.69333333,\n",
       "       0.50693289, 0.56935191, 0.52232667, 0.65621156, 0.47920133,\n",
       "       0.55777921, 0.72834646, 0.57142857, 0.78868552, 0.53622362,\n",
       "       0.50757117, 0.508125  , 0.42422434, 0.66141732, 0.58449425,\n",
       "       0.77614571, 0.63663484, 0.61417323, 0.64507937, 0.46478092,\n",
       "       0.42857143, 0.42675159, 0.5213919 , 0.65621156, 0.41312336,\n",
       "       0.54857143, 0.60947109, 0.53136531, 0.603125  , 0.60684312,\n",
       "       0.62293212, 0.50713063, 0.54534606, 0.4346834 , 0.50757117,\n",
       "       0.56935191, 0.62690952, 0.47625   , 0.65155131, 0.45725534,\n",
       "       0.65155131, 0.714375  , 0.64159812, 0.50656168, 0.52924146,\n",
       "       0.45957447, 0.63052695, 0.55048488, 0.42448759, 0.43649473,\n",
       "       0.5900581 , 0.80374435, 0.54274466, 0.69689737, 0.603125  ,\n",
       "       0.56698002, 0.4346834 , 0.52049622, 0.54      , 0.62920635,\n",
       "       0.48508353, 0.57577566, 0.56698002, 0.56085919, 0.52924146,\n",
       "       0.51406074, 0.49968254, 0.60024228, 0.57957787, 0.54099419,\n",
       "       0.508125  , 0.58449425, 0.70584829, 0.62690952, 0.60973983,\n",
       "       0.61538462, 0.59694477, 0.650625  , 0.71021107, 0.44574468,\n",
       "       0.635     , 0.52712766, 0.55877244, 0.75775656, 0.46062176,\n",
       "       0.873125  , 0.70745003, 0.50713063, 0.52868391, 0.46601942,\n",
       "       0.52868391, 0.73431734, 0.75899525, 0.70295203, 0.5653166 ,\n",
       "       0.72175597, 0.54      , 0.69230769, 0.70984127, 0.64736537,\n",
       "       0.62007765, 0.64736537, 0.52375   , 0.635     , 0.5968254 ,\n",
       "       0.70497095, 0.60867085, 0.61417323, 0.62690952, 0.46957041,\n",
       "       0.54042553, 0.77396825, 0.48400984, 0.66184134, 0.68198091,\n",
       "       0.53701528, 0.49236193, 0.55229142, 0.52924146, 0.52924146,\n",
       "       0.508125  , 0.55229142, 0.52755906, 0.47579083, 0.76250834,\n",
       "       0.62112172, 0.62690952, 0.6520251 , 0.84973753, 0.52375   ,\n",
       "       0.55048488, 0.761875  , 0.54      , 0.69230769, 0.63976759,\n",
       "       0.67778519, 0.55736783, 0.66141732, 0.59348093, 0.5653166 ,\n",
       "       0.66095238, 0.666875  , 0.63776383, 0.64736537, 0.60620525,\n",
       "       0.56935191, 0.50806972, 0.45325953, 0.58333333, 0.58275501,\n",
       "       0.57577566, 0.57125   , 0.60620525, 0.5233192 , 0.55736783,\n",
       "       0.66222962, 0.70984127, 0.56211562, 0.650625  , 0.56085919,\n",
       "       0.59306204, 0.68307087, 0.50806972, 0.57391866, 0.82222222,\n",
       "       0.53701528, 0.49236193, 0.55229142, 0.52924146, 0.52924146,\n",
       "       0.508125  , 0.55229142, 0.52755906, 0.47579083, 0.76250834,\n",
       "       0.62112172, 0.62690952, 0.6520251 , 0.84973753, 0.52375   ,\n",
       "       0.55048488, 0.761875  , 0.54      , 0.69230769, 0.63976759,\n",
       "       0.67778519, 0.55736783, 0.66141732, 0.59348093, 0.5653166 ,\n",
       "       0.66095238, 0.666875  , 0.63776383, 0.64736537, 0.60620525,\n",
       "       0.56935191, 0.50806972, 0.45325953, 0.58333333, 0.50757117,\n",
       "       0.57577566, 0.57125   , 0.60620525, 0.5233192 , 0.55736783,\n",
       "       0.66222962, 0.51537515, 0.71832718, 0.55877244, 0.47803765,\n",
       "       0.51619048, 0.52375   , 0.7124105 , 0.43028978, 0.49968254,\n",
       "       0.50757117, 0.66628637, 0.48605796, 0.59704757, 0.62112172,\n",
       "       0.58031746, 0.44639612, 0.70152761, 0.44770858, 0.5       ,\n",
       "       0.61163337, 0.56350527, 0.85460317, 0.50757117, 0.5968254 ,\n",
       "       0.714375  , 0.55736783, 0.56935191, 0.67746032, 0.75356532,\n",
       "       0.43973747, 0.54884684, 0.49998502, 0.43028978, 0.61538462,\n",
       "       0.50693289, 0.56148867, 0.56692913, 0.54      , 0.5213533 ,\n",
       "       0.46957041, 0.485069  , 0.49972663, 0.43244681, 0.5653166 ,\n",
       "       0.57737105, 0.59694477, 0.444375  , 0.54674047, 0.64780921,\n",
       "       0.45142857, 0.53042959, 0.67716535, 0.70438159, 0.70497095,\n",
       "       0.47238928, 0.66141732, 0.42124352, 0.63776383, 0.54099419,\n",
       "       0.48508353, 0.74625   , 0.66666667, 0.49306711, 0.61269841,\n",
       "       0.41100324, 0.53346457, 0.56916348, 0.66648442, 0.47131609])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## imputed not categorized parameters.\n",
    "#waist == c12_not_categ\n",
    "#stature   == x[:,6]\n",
    "whs = np.divide(c12_not_categ , x[:,6])\n",
    "whs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G5aUSUNwGRF",
    "outputId": "90fcc445-0e82-478b-f412-006601b698c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 0., 0., 2., 0., 0., 0., 2., 1., 3., 3., 0., 0., 3., 0.,\n",
       "       3., 0., 1., 3., 0., 1., 3., 3., 3., 0., 3., 0., 3., 1., 0., 3., 3.,\n",
       "       0., 3., 0., 1., 2., 3., 0., 0., 0., 2., 3., 0., 3., 1., 0., 3., 2.,\n",
       "       2., 0., 1., 3., 0., 2., 3., 1., 0., 0., 1., 3., 1., 0., 1., 1., 3.,\n",
       "       0., 0., 0., 3., 0., 2., 3., 2., 0., 3., 0., 0., 3., 2., 3., 0., 2.,\n",
       "       3., 1., 2., 3., 3., 0., 0., 1., 0., 3., 0., 2., 2., 1., 1., 0., 0.,\n",
       "       0., 0., 2., 2., 2., 3., 1., 3., 0., 1., 0., 0., 1., 0., 3., 2., 2.,\n",
       "       0., 1., 1., 3., 3., 0., 3., 3., 2., 3., 1., 1., 2., 3., 1., 3., 0.,\n",
       "       3., 3., 0., 3., 3., 1., 0., 1., 3., 2., 1., 2., 2., 1., 2., 3., 3.,\n",
       "       3., 3., 1., 3., 3., 2., 3., 0., 1., 0., 0., 0., 3., 0., 0., 2., 3.,\n",
       "       2., 0., 0., 3., 2., 2., 1., 2., 3., 3., 3., 2., 0., 2., 3., 1., 2.,\n",
       "       3., 3., 0., 0., 3., 2., 2., 3., 0., 2., 0., 3., 0., 0., 2., 3., 3.,\n",
       "       0., 3., 2., 2., 3., 0., 1., 3., 1., 2., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       3., 3., 3., 3., 0., 2., 1., 3., 0., 1., 3., 2., 3., 1., 0., 0., 0.,\n",
       "       3., 2., 3., 3., 3., 3., 0., 0., 0., 1., 3., 0., 1., 3., 1., 3., 3.,\n",
       "       3., 0., 1., 0., 0., 2., 3., 0., 3., 0., 3., 3., 3., 0., 1., 0., 3.,\n",
       "       1., 0., 0., 2., 3., 1., 3., 3., 2., 0., 1., 1., 3., 0., 2., 2., 2.,\n",
       "       1., 1., 0., 3., 2., 1., 0., 2., 3., 3., 3., 3., 2., 3., 3., 0., 3.,\n",
       "       1., 1., 3., 0., 3., 3., 0., 1., 0., 1., 3., 3., 3., 2., 3., 1., 3.,\n",
       "       3., 3., 3., 3., 1., 3., 2., 3., 3., 3., 3., 0., 1., 3., 0., 3., 3.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 0., 3., 3., 3., 3., 3., 1., 1., 3.,\n",
       "       1., 3., 3., 3., 1., 3., 2., 2., 3., 3., 3., 3., 3., 2., 0., 0., 2.,\n",
       "       2., 2., 2., 3., 1., 1., 3., 3., 2., 3., 2., 2., 3., 0., 2., 3., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 0., 3., 3., 3., 3., 3., 1., 1., 3., 1.,\n",
       "       3., 3., 3., 1., 3., 2., 2., 3., 3., 3., 3., 3., 2., 0., 0., 2., 0.,\n",
       "       2., 2., 3., 1., 1., 3., 1., 3., 1., 0., 1., 1., 3., 0., 0., 0., 3.,\n",
       "       0., 2., 3., 2., 0., 3., 0., 0., 3., 2., 3., 0., 2., 3., 1., 2., 3.,\n",
       "       3., 0., 1., 0., 0., 3., 0., 2., 2., 1., 1., 0., 0., 0., 0., 2., 2.,\n",
       "       2., 0., 1., 3., 0., 1., 3., 3., 3., 0., 3., 0., 3., 1., 0., 3., 3.,\n",
       "       0., 3., 0., 1., 2., 3., 0.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#categorize whs\n",
    "whs[whs >= 0.60 ] = 3\n",
    "whs[np.logical_and(whs < 0.6, whs >= 0.56)] = 2\n",
    "whs[np.logical_and(whs < 0.56, whs >= 0.51)] = 1\n",
    "whs[whs < 0.51 ] = 0\n",
    "whs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2PzAjXzwGRG"
   },
   "outputs": [],
   "source": [
    "##concatenate new feature to x\n",
    "y_whs = whs.reshape(len(whs),1)\n",
    "x = np.concatenate((x, y_whs), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqRB2AsJwGRG",
    "outputId": "1e41e12b-7aef-41d6-c379-e38615799c75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,   0. ,   1. , ...,  96.5,   1. ,   1. ],\n",
       "       [  0. ,   0. ,   1. , ..., 101.6,   0. ,   1. ],\n",
       "       [  1. ,   0. ,   1. , ..., 114.3,   2. ,   2. ],\n",
       "       ...,\n",
       "       [  2. ,   0. ,   0. , ..., 111.8,   3. ,   2. ],\n",
       "       [  1. ,   0. ,   1. , ..., 124.5,   3. ,   3. ],\n",
       "       [  1. ,   0. ,   0. , ...,  99.1,   1. ,   0. ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkwLxDirfW_H"
   },
   "source": [
    "# imputing coulmn DV (Diabetes)\n",
    "In the last stage of data pre-processing, diabetes, or no diabetes as dependent variable changes to 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU3Fqh7YfW_H"
   },
   "source": [
    "zero values : 0\n",
    "\n",
    "null values : 7\n",
    "\n",
    "standard range : checked by medical team\n",
    "\n",
    "classification : X\n",
    "\n",
    "outlires : X\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyMB1IUZfW_I"
   },
   "source": [
    "# imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTZZEhV5fW_I",
    "outputId": "9b82cc8c-fbe3-42fd-a7a2-8a60aa4eeaca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'Diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'Diabetes', 'Diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'Diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'Diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'Diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'Diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'Diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'Diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'Diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes',\n",
       "       'No diabetes', 'No diabetes', 'No diabetes', 'No diabetes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan , strategy = 'most_frequent')\n",
    "cdv = np.reshape(y, (-1, 1))\n",
    "imputer.fit(cdv) #2D\n",
    "cdv = imputer.transform(cdv)\n",
    "y = cdv.flatten() #1D\n",
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnEvCGIAyGyk"
   },
   "source": [
    "## label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATO-djGMyOCm",
    "outputId": "d05a625c-0e1e-44d8-dd44-f0bc239d3437"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_y = LabelEncoder()\n",
    "y =  le_y.fit_transform(y)\n",
    "y\n",
    "# 1 == No diabetes\n",
    "# 0 == Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9lFwGAohKIw"
   },
   "source": [
    "# Train,Test split and Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCyb4CVPjZIw"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3DDOEgdhSKo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "puX6xwIjjjZW",
    "outputId": "9c4b0cb8-2f16-44c5-8292-b9089e61f59c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-aa6f9e7c-2837-4e2a-9b5e-405cb1b08ba9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.1</td>\n",
       "      <td>65.83</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.1</td>\n",
       "      <td>73.09</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>84.44</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>129.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>131.21</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.8</td>\n",
       "      <td>67.19</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>101.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.5</td>\n",
       "      <td>66.28</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>119.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.5</td>\n",
       "      <td>63.56</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>167.6</td>\n",
       "      <td>71.73</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>81.72</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>111.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.5</td>\n",
       "      <td>97.61</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>116.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 16 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa6f9e7c-2837-4e2a-9b5e-405cb1b08ba9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-aa6f9e7c-2837-4e2a-9b5e-405cb1b08ba9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-aa6f9e7c-2837-4e2a-9b5e-405cb1b08ba9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      0    1    2    3    4    5      6       7      8    9    10   11   12  \\\n",
       "0    1.0  0.0  1.0  1.0  1.0  0.0  165.1   65.83   66.0  1.0  1.0  1.0  3.0   \n",
       "1    0.0  0.0  2.0  0.0  0.0  0.0  165.1   73.09   56.0  2.0  0.0  0.0  0.0   \n",
       "2    1.0  1.0  0.0  0.0  2.0  0.0  160.0   84.44  111.0  3.0  1.0  1.0  3.0   \n",
       "3    0.0  0.0  2.0  2.0  2.0  1.0  170.2  131.21   54.0  5.0  0.0  0.0  1.0   \n",
       "4    1.0  0.0  1.0  1.0  2.0  1.0  177.8   67.19   82.0  1.0  1.0  1.0  3.0   \n",
       "..   ...  ...  ...  ...  ...  ...    ...     ...    ...  ...  ...  ...  ...   \n",
       "370  1.0  0.0  1.0  0.0  1.0  0.0  157.5   66.28   76.0  2.0  1.0  1.0  3.0   \n",
       "371  2.0  0.0  1.0  1.0  1.0  0.0  157.5   63.56   72.0  2.0  1.0  1.0  2.0   \n",
       "372  2.0  1.0  2.0  2.0  0.0  1.0  167.6   71.73   81.0  2.0  1.0  1.0  3.0   \n",
       "373  1.0  0.0  0.0  0.0  0.0  0.0  170.2   81.72   84.0  2.0  0.0  1.0  2.0   \n",
       "374  0.0  0.0  1.0  1.0  1.0  0.0  190.5   97.61   84.0  2.0  0.0  1.0  3.0   \n",
       "\n",
       "        13   14   15  \n",
       "0    106.7  2.0  2.0  \n",
       "1     94.0  0.0  0.0  \n",
       "2    129.5  3.0  3.0  \n",
       "3     96.5  2.0  0.0  \n",
       "4    101.6  3.0  1.0  \n",
       "..     ...  ...  ...  \n",
       "370  119.4  3.0  3.0  \n",
       "371  109.2  1.0  2.0  \n",
       "372  106.7  2.0  2.0  \n",
       "373  111.8  0.0  1.0  \n",
       "374  116.8  0.0  0.0  \n",
       "\n",
       "[375 rows x 16 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQSevBiSjnof",
    "outputId": "f0700823-c3d0-4335-9396-4516b80ec731"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8fAl2gDmH5x"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqDnlw313bYG",
    "outputId": "99c6cb53-5ce6-474e-8562-d389d6afff13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165.1 ,  65.83,  66.  , 106.7 ],\n",
       "       [165.1 ,  73.09,  56.  ,  94.  ],\n",
       "       [160.  ,  84.44, 111.  , 129.5 ],\n",
       "       ...,\n",
       "       [167.6 ,  71.73,  81.  , 106.7 ],\n",
       "       [170.2 ,  81.72,  84.  , 111.8 ],\n",
       "       [190.5 ,  97.61,  84.  , 116.8 ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,[6,7,8,13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF-DLoTjmOSu"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:,[6,7,8,13]] = sc.fit_transform(X_train[:,[6,7,8,13]])\n",
    "X_test[:,[6,7,8,13]] = sc.transform(X_test[:,[6,7,8,13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sMSp52Dj40Xw",
    "outputId": "7a4fb783-c87f-440b-852b-f6bd5fe625c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0f64bb72-8d31-467a-bf25-0cbc95a99a68\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.303998</td>\n",
       "      <td>-0.776778</td>\n",
       "      <td>-0.788704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.140416</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.303998</td>\n",
       "      <td>-0.371302</td>\n",
       "      <td>-1.339131</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.023927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.813766</td>\n",
       "      <td>0.262603</td>\n",
       "      <td>1.688219</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.445731</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205770</td>\n",
       "      <td>2.874738</td>\n",
       "      <td>-1.449216</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.850007</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965425</td>\n",
       "      <td>-0.700821</td>\n",
       "      <td>0.091980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.495212</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.063652</td>\n",
       "      <td>-0.751645</td>\n",
       "      <td>-0.238276</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.743096</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.063652</td>\n",
       "      <td>-0.903559</td>\n",
       "      <td>-0.458447</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.054111</td>\n",
       "      <td>-0.447259</td>\n",
       "      <td>0.036937</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.140416</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205770</td>\n",
       "      <td>0.110689</td>\n",
       "      <td>0.202066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.214380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.234848</td>\n",
       "      <td>0.998156</td>\n",
       "      <td>0.202066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.562219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 16 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f64bb72-8d31-467a-bf25-0cbc95a99a68')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0f64bb72-8d31-467a-bf25-0cbc95a99a68 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0f64bb72-8d31-467a-bf25-0cbc95a99a68');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      0    1    2    3    4    5         6         7         8    9    10  \\\n",
       "0    1.0  0.0  1.0  1.0  1.0  0.0 -0.303998 -0.776778 -0.788704  1.0  1.0   \n",
       "1    0.0  0.0  2.0  0.0  0.0  0.0 -0.303998 -0.371302 -1.339131  2.0  0.0   \n",
       "2    1.0  1.0  0.0  0.0  2.0  0.0 -0.813766  0.262603  1.688219  3.0  1.0   \n",
       "3    0.0  0.0  2.0  2.0  2.0  1.0  0.205770  2.874738 -1.449216  5.0  0.0   \n",
       "4    1.0  0.0  1.0  1.0  2.0  1.0  0.965425 -0.700821  0.091980  1.0  1.0   \n",
       "..   ...  ...  ...  ...  ...  ...       ...       ...       ...  ...  ...   \n",
       "370  1.0  0.0  1.0  0.0  1.0  0.0 -1.063652 -0.751645 -0.238276  2.0  1.0   \n",
       "371  2.0  0.0  1.0  1.0  1.0  0.0 -1.063652 -0.903559 -0.458447  2.0  1.0   \n",
       "372  2.0  1.0  2.0  2.0  0.0  1.0 -0.054111 -0.447259  0.036937  2.0  1.0   \n",
       "373  1.0  0.0  0.0  0.0  0.0  0.0  0.205770  0.110689  0.202066  2.0  0.0   \n",
       "374  0.0  0.0  1.0  1.0  1.0  0.0  2.234848  0.998156  0.202066  2.0  0.0   \n",
       "\n",
       "      11   12        13   14   15  \n",
       "0    1.0  3.0 -0.140416  2.0  2.0  \n",
       "1    0.0  0.0 -1.023927  0.0  0.0  \n",
       "2    1.0  3.0  1.445731  3.0  3.0  \n",
       "3    0.0  1.0 -0.850007  2.0  0.0  \n",
       "4    1.0  3.0 -0.495212  3.0  1.0  \n",
       "..   ...  ...       ...  ...  ...  \n",
       "370  1.0  3.0  0.743096  3.0  3.0  \n",
       "371  1.0  2.0  0.033504  1.0  2.0  \n",
       "372  1.0  3.0 -0.140416  2.0  2.0  \n",
       "373  1.0  2.0  0.214380  0.0  1.0  \n",
       "374  1.0  3.0  0.562219  0.0  0.0  \n",
       "\n",
       "[375 rows x 16 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujfanMoh4-NN",
    "outputId": "2cf6343b-677c-4922-a5a8-cc9afc5b88f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  1.        , ..., -1.37872283,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        , ..., -0.49521155,\n",
       "         3.        ,  3.        ],\n",
       "       [ 1.        ,  0.        ,  1.        , ...,  0.21438019,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 1.        ,  2.        ,  2.        , ...,  1.271811  ,\n",
       "         1.        ,  3.        ],\n",
       "       [ 1.        ,  0.        ,  1.        , ...,  0.56221927,\n",
       "         2.        ,  3.        ],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.21438019,\n",
       "         2.        ,  2.        ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHgwnV7Z5xhN"
   },
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HRf9X5l54sW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHoDsbjT52Bz"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLsxjDoI5_q-"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "In0jTsOe6SDu",
    "outputId": "b3fc4785-a586-4e32-86d9-f401b1708dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score achieved using Logistic Regression is: 87.2 %\n"
     ]
    }
   ],
   "source": [
    "score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlIjGc8VGAxe",
    "outputId": "dc59b560-3e61-4919-d218-1201dfae2f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 14]\n",
      " [ 2 96]]\n",
      "correctly diagnose nodiabetics is 13/15\n",
      "correctly diagnose diabetics is 96/110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con1 = confusion_matrix(Y_test, Y_pred_lr)\n",
    "print(con1)\n",
    "print(f\"correctly diagnose nodiabetics is {con1[0,0]}/{con1[1,0]+con1[0,0]}\")\n",
    "print(f\"correctly diagnose diabetics is {con1[1,1]}/{con1[0,1]+con1[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUeCaOqe6gZN"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKl-XKv-6jxF"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv = svm.SVC(kernel='linear')\n",
    "\n",
    "sv.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_svm = sv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUqhzjVp6mAF",
    "outputId": "b6f8206c-bd4e-4d8b-ab6c-a7e927886b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score achieved using Linear SVM is: 87.2 %\n"
     ]
    }
   ],
   "source": [
    "score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Linear SVM is: \"+str(score_svm)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpJFESQGNklO",
    "outputId": "293c69d7-5968-45a9-ff17-b73b82d33b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 14]\n",
      " [ 2 96]]\n",
      "correctly diagnose nodiabetics is 13/15\n",
      "correctly diagnose diabetics is 96/110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con2 = confusion_matrix(Y_test, Y_pred_svm)\n",
    "print(con2)\n",
    "print(f\"correctly diagnose nodiabetics is {con2[0,0]}/{con2[1,0]+con2[0,0]}\")\n",
    "print(f\"correctly diagnose diabetics is {con2[1,1]}/{con2[0,1]+con2[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UcUhXR76zqt"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6u9pQo665t2"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "\n",
    "for x in range(200):\n",
    "    dt = DecisionTreeClassifier(random_state=x)\n",
    "    dt.fit(X_train,Y_train)\n",
    "    Y_pred_dt = dt.predict(X_test)\n",
    "    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n",
    "    if(current_accuracy>max_accuracy):\n",
    "        max_accuracy = current_accuracy\n",
    "        best_x = x\n",
    "        \n",
    "#print(max_accuracy)\n",
    "#print(best_x)\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=best_x)\n",
    "dt.fit(X_train,Y_train)\n",
    "Y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGRn6dGH68p-",
    "outputId": "d4988a82-c6bc-4ac0-8155-3b4b897ef67c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score achieved using Decision Tree is: 95.2 %\n"
     ]
    }
   ],
   "source": [
    "score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H01psppXQyp8",
    "outputId": "fcb0f6d8-c843-4c9f-db73-d8a545adb0db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  4]\n",
      " [ 2 96]]\n",
      "correctly diagnose nodiabetics is 23/25\n",
      "correctly diagnose diabetics is 96/100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con3 = confusion_matrix(Y_test, Y_pred_dt)\n",
    "print(con3)\n",
    "print(f\"correctly diagnose nodiabetics is {con3[0,0]}/{con3[1,0]+con3[0,0]}\")\n",
    "print(f\"correctly diagnose diabetics is {con3[1,1]}/{con3[0,1]+con3[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBf3uF3b6_8e"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrWeCcar7DU-"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "\n",
    "for x in range(200):\n",
    "    rf = RandomForestClassifier(random_state=x)\n",
    "    rf.fit(X_train,Y_train)\n",
    "    Y_pred_rf = rf.predict(X_test)\n",
    "    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)\n",
    "    if(current_accuracy>max_accuracy):\n",
    "        max_accuracy = current_accuracy\n",
    "        best_x = x\n",
    "        \n",
    "#print(max_accuracy)\n",
    "#print(best_x)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=best_x)\n",
    "rf.fit(X_train,Y_train)\n",
    "Y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChiGmRPm7L4u",
    "outputId": "cb53f0d6-589c-4f39-ef13-b73cecd4d569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score achieved using Decision Tree is: 92.8 %\n"
     ]
    }
   ],
   "source": [
    "score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Decision Tree is: \"+str(score_rf)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5GaDnLiRGAk",
    "outputId": "44583e12-57a9-47ed-b444-613d5a879090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  8]\n",
      " [ 1 97]]\n",
      "correctly diagnose nodiabetics is 19/20\n",
      "correctly diagnose diabetics is 97/105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con4 = confusion_matrix(Y_test, Y_pred_rf)\n",
    "print(con4)\n",
    "print(f\"correctly diagnose nodiabetics is {con4[0,0]}/{con4[1,0]+con4[0,0]}\")\n",
    "print(f\"correctly diagnose diabetics is {con4[1,1]}/{con4[0,1]+con4[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYPeO2D77NlV"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8XABdy67RFu"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85lmwPC77Upn"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(11,activation='relu',input_dim=16))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xY6OiZFP7XOQ",
    "outputId": "3ea5491e-aa28-4d87-9505-ada0fa06b43b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0487 - accuracy: 0.9813\n",
      "Epoch 2/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0485 - accuracy: 0.9813\n",
      "Epoch 3/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0488 - accuracy: 0.9813\n",
      "Epoch 4/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 0.9813\n",
      "Epoch 5/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9813\n",
      "Epoch 6/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 0.9813\n",
      "Epoch 7/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0480 - accuracy: 0.9813\n",
      "Epoch 8/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 0.9813\n",
      "Epoch 9/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 0.9840\n",
      "Epoch 10/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9813\n",
      "Epoch 11/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9813\n",
      "Epoch 12/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9813\n",
      "Epoch 13/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9813\n",
      "Epoch 14/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9813\n",
      "Epoch 15/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0487 - accuracy: 0.9787\n",
      "Epoch 16/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.9813\n",
      "Epoch 17/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0473 - accuracy: 0.9813\n",
      "Epoch 18/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9813\n",
      "Epoch 19/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9813\n",
      "Epoch 20/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0473 - accuracy: 0.9813\n",
      "Epoch 21/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9813\n",
      "Epoch 22/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9813\n",
      "Epoch 23/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9813\n",
      "Epoch 24/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9813\n",
      "Epoch 25/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9813\n",
      "Epoch 26/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9813\n",
      "Epoch 27/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9813\n",
      "Epoch 28/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0471 - accuracy: 0.9813\n",
      "Epoch 29/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 0.9813\n",
      "Epoch 30/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 0.9813\n",
      "Epoch 31/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 0.9813\n",
      "Epoch 32/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 0.9813\n",
      "Epoch 33/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9813\n",
      "Epoch 34/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9813\n",
      "Epoch 35/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9787\n",
      "Epoch 36/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9813\n",
      "Epoch 37/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 0.9813\n",
      "Epoch 38/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0463 - accuracy: 0.9813\n",
      "Epoch 39/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9813\n",
      "Epoch 40/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 0.9813\n",
      "Epoch 41/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9813\n",
      "Epoch 42/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.9813\n",
      "Epoch 43/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9813\n",
      "Epoch 44/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.9840\n",
      "Epoch 45/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9813\n",
      "Epoch 46/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.9813\n",
      "Epoch 47/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0459 - accuracy: 0.9813\n",
      "Epoch 48/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9813\n",
      "Epoch 49/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9840\n",
      "Epoch 50/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0454 - accuracy: 0.9813\n",
      "Epoch 51/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0454 - accuracy: 0.9813\n",
      "Epoch 52/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9813\n",
      "Epoch 53/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9840\n",
      "Epoch 54/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9840\n",
      "Epoch 55/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9813\n",
      "Epoch 56/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9813\n",
      "Epoch 57/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9840\n",
      "Epoch 58/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9840\n",
      "Epoch 59/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9813\n",
      "Epoch 60/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9813\n",
      "Epoch 61/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9840\n",
      "Epoch 62/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9813\n",
      "Epoch 63/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9813\n",
      "Epoch 64/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.9840\n",
      "Epoch 65/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0456 - accuracy: 0.9813\n",
      "Epoch 66/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9787\n",
      "Epoch 67/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9840\n",
      "Epoch 68/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0446 - accuracy: 0.9813\n",
      "Epoch 69/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0446 - accuracy: 0.9813\n",
      "Epoch 70/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0443 - accuracy: 0.9813\n",
      "Epoch 71/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0441 - accuracy: 0.9840\n",
      "Epoch 72/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9813\n",
      "Epoch 73/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9840\n",
      "Epoch 74/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9840\n",
      "Epoch 75/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9867\n",
      "Epoch 76/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9840\n",
      "Epoch 77/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9840\n",
      "Epoch 78/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9813\n",
      "Epoch 79/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.9813\n",
      "Epoch 80/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 0.9840\n",
      "Epoch 81/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9840\n",
      "Epoch 82/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0447 - accuracy: 0.9840\n",
      "Epoch 83/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9840\n",
      "Epoch 84/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.9840\n",
      "Epoch 85/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9813\n",
      "Epoch 86/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0435 - accuracy: 0.9840\n",
      "Epoch 87/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0433 - accuracy: 0.9840\n",
      "Epoch 88/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9813\n",
      "Epoch 89/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9813\n",
      "Epoch 90/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0433 - accuracy: 0.9840\n",
      "Epoch 91/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9867\n",
      "Epoch 92/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9840\n",
      "Epoch 93/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9867\n",
      "Epoch 94/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9840\n",
      "Epoch 95/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9813\n",
      "Epoch 96/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9840\n",
      "Epoch 97/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9867\n",
      "Epoch 98/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9840\n",
      "Epoch 99/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.9840\n",
      "Epoch 100/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0427 - accuracy: 0.9867\n",
      "Epoch 101/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 0.9813\n",
      "Epoch 102/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0428 - accuracy: 0.9840\n",
      "Epoch 103/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9840\n",
      "Epoch 104/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9813\n",
      "Epoch 105/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9840\n",
      "Epoch 106/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0428 - accuracy: 0.9867\n",
      "Epoch 107/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0425 - accuracy: 0.9840\n",
      "Epoch 108/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9813\n",
      "Epoch 109/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9867\n",
      "Epoch 110/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9840\n",
      "Epoch 111/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9867\n",
      "Epoch 112/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0419 - accuracy: 0.9840\n",
      "Epoch 113/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0419 - accuracy: 0.9840\n",
      "Epoch 114/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9813\n",
      "Epoch 115/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9813\n",
      "Epoch 116/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9840\n",
      "Epoch 117/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9840\n",
      "Epoch 118/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0417 - accuracy: 0.9867\n",
      "Epoch 119/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9867\n",
      "Epoch 120/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9867\n",
      "Epoch 121/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0419 - accuracy: 0.9813\n",
      "Epoch 122/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.9813\n",
      "Epoch 123/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0418 - accuracy: 0.9867\n",
      "Epoch 124/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 0.9840\n",
      "Epoch 125/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0417 - accuracy: 0.9840\n",
      "Epoch 126/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9867\n",
      "Epoch 127/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9840\n",
      "Epoch 128/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 0.9840\n",
      "Epoch 129/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9840\n",
      "Epoch 130/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9840\n",
      "Epoch 131/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9867\n",
      "Epoch 132/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9867\n",
      "Epoch 133/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9867\n",
      "Epoch 134/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9867\n",
      "Epoch 135/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9867\n",
      "Epoch 136/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9867\n",
      "Epoch 137/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9840\n",
      "Epoch 138/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9867\n",
      "Epoch 139/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9867\n",
      "Epoch 140/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 0.9840\n",
      "Epoch 141/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9840\n",
      "Epoch 142/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9867\n",
      "Epoch 143/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9867\n",
      "Epoch 144/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9867\n",
      "Epoch 145/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9867\n",
      "Epoch 146/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.9867\n",
      "Epoch 147/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9867\n",
      "Epoch 148/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9840\n",
      "Epoch 149/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9867\n",
      "Epoch 150/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9867\n",
      "Epoch 151/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9867\n",
      "Epoch 152/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0399 - accuracy: 0.9867\n",
      "Epoch 153/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9867\n",
      "Epoch 154/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9840\n",
      "Epoch 155/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0399 - accuracy: 0.9840\n",
      "Epoch 156/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9867\n",
      "Epoch 157/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9840\n",
      "Epoch 158/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9867\n",
      "Epoch 159/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9867\n",
      "Epoch 160/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9867\n",
      "Epoch 161/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9867\n",
      "Epoch 162/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9840\n",
      "Epoch 163/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.9867\n",
      "Epoch 164/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9840\n",
      "Epoch 165/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9867\n",
      "Epoch 166/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9867\n",
      "Epoch 167/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9867\n",
      "Epoch 168/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9813\n",
      "Epoch 169/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9813\n",
      "Epoch 170/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.9893\n",
      "Epoch 171/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9867\n",
      "Epoch 172/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9867\n",
      "Epoch 173/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0399 - accuracy: 0.9840\n",
      "Epoch 174/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9867\n",
      "Epoch 175/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.9867\n",
      "Epoch 176/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9867\n",
      "Epoch 177/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9867\n",
      "Epoch 178/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9867\n",
      "Epoch 179/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.9867\n",
      "Epoch 180/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.9867\n",
      "Epoch 181/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9867\n",
      "Epoch 182/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9867\n",
      "Epoch 183/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.9840\n",
      "Epoch 184/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9840\n",
      "Epoch 185/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9867\n",
      "Epoch 186/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9867\n",
      "Epoch 187/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0386 - accuracy: 0.9867\n",
      "Epoch 188/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9867\n",
      "Epoch 189/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.9867\n",
      "Epoch 190/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9867\n",
      "Epoch 191/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0386 - accuracy: 0.9867\n",
      "Epoch 192/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9867\n",
      "Epoch 193/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9867\n",
      "Epoch 194/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9867\n",
      "Epoch 195/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0386 - accuracy: 0.9893\n",
      "Epoch 196/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0382 - accuracy: 0.9867\n",
      "Epoch 197/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0388 - accuracy: 0.9840\n",
      "Epoch 198/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0381 - accuracy: 0.9840\n",
      "Epoch 199/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9867\n",
      "Epoch 200/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.9867\n",
      "Epoch 201/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.9840\n",
      "Epoch 202/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0378 - accuracy: 0.9867\n",
      "Epoch 203/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0386 - accuracy: 0.9867\n",
      "Epoch 204/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9840\n",
      "Epoch 205/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.9840\n",
      "Epoch 206/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0381 - accuracy: 0.9893\n",
      "Epoch 207/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9840\n",
      "Epoch 208/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.9867\n",
      "Epoch 209/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0378 - accuracy: 0.9867\n",
      "Epoch 210/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9867\n",
      "Epoch 211/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9867\n",
      "Epoch 212/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9867\n",
      "Epoch 213/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9867\n",
      "Epoch 214/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9867\n",
      "Epoch 215/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9867\n",
      "Epoch 216/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9867\n",
      "Epoch 217/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9867\n",
      "Epoch 218/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9893\n",
      "Epoch 219/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9893\n",
      "Epoch 220/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9867\n",
      "Epoch 221/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9840\n",
      "Epoch 222/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.9867\n",
      "Epoch 223/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9893\n",
      "Epoch 224/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9867\n",
      "Epoch 225/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9867\n",
      "Epoch 226/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9813\n",
      "Epoch 227/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9867\n",
      "Epoch 228/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9867\n",
      "Epoch 229/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9867\n",
      "Epoch 230/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0369 - accuracy: 0.9867\n",
      "Epoch 231/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9840\n",
      "Epoch 232/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9893\n",
      "Epoch 233/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9867\n",
      "Epoch 234/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9893\n",
      "Epoch 235/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9867\n",
      "Epoch 236/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9867\n",
      "Epoch 237/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9893\n",
      "Epoch 238/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.9813\n",
      "Epoch 239/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9840\n",
      "Epoch 240/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.9840\n",
      "Epoch 241/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9867\n",
      "Epoch 242/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9867\n",
      "Epoch 243/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.9893\n",
      "Epoch 244/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9893\n",
      "Epoch 245/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9840\n",
      "Epoch 246/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9893\n",
      "Epoch 247/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9867\n",
      "Epoch 248/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9813\n",
      "Epoch 249/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9840\n",
      "Epoch 250/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9840\n",
      "Epoch 251/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9840\n",
      "Epoch 252/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9840\n",
      "Epoch 253/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9867\n",
      "Epoch 254/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9893\n",
      "Epoch 255/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9840\n",
      "Epoch 256/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9867\n",
      "Epoch 257/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9840\n",
      "Epoch 258/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9893\n",
      "Epoch 259/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.9893\n",
      "Epoch 260/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0358 - accuracy: 0.9867\n",
      "Epoch 261/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 0.9867\n",
      "Epoch 262/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9867\n",
      "Epoch 263/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0358 - accuracy: 0.9893\n",
      "Epoch 264/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 0.9893\n",
      "Epoch 265/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.9867\n",
      "Epoch 266/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9840\n",
      "Epoch 267/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0356 - accuracy: 0.9867\n",
      "Epoch 268/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9893\n",
      "Epoch 269/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0358 - accuracy: 0.9840\n",
      "Epoch 270/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9840\n",
      "Epoch 271/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9893\n",
      "Epoch 272/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9867\n",
      "Epoch 273/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.9840\n",
      "Epoch 274/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.9893\n",
      "Epoch 275/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9840\n",
      "Epoch 276/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9893\n",
      "Epoch 277/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9840\n",
      "Epoch 278/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0352 - accuracy: 0.9867\n",
      "Epoch 279/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9893\n",
      "Epoch 280/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9893\n",
      "Epoch 281/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0350 - accuracy: 0.9840\n",
      "Epoch 282/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9893\n",
      "Epoch 283/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9893\n",
      "Epoch 284/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9893\n",
      "Epoch 285/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9840\n",
      "Epoch 286/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9893\n",
      "Epoch 287/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9893\n",
      "Epoch 288/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9893\n",
      "Epoch 289/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9893\n",
      "Epoch 290/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9893\n",
      "Epoch 291/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9813\n",
      "Epoch 292/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 0.9840\n",
      "Epoch 293/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9893\n",
      "Epoch 294/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9893\n",
      "Epoch 295/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9840\n",
      "Epoch 296/300\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9893\n",
      "Epoch 297/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9893\n",
      "Epoch 298/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 0.9893\n",
      "Epoch 299/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9840\n",
      "Epoch 300/300\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80237e3250>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFazfuQa7dE9",
    "outputId": "7b868c98-d74b-4eb3-d156-23c2845fc040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred_nn = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdYGYONA7gj-"
   },
   "outputs": [],
   "source": [
    "rounded = [round(x[0]) for x in Y_pred_nn]\n",
    "\n",
    "Y_pred_nn = rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXpQwG2E7jMO",
    "outputId": "e047a334-b9c2-4da4-a70b-f10c26b811d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score achieved using Neural Network is: 86.4 %\n"
     ]
    }
   ],
   "source": [
    "score_nn = round(accuracy_score(Y_pred_nn,Y_test)*100,2)\n",
    "\n",
    "print(\"The accuracy score achieved using Neural Network is: \"+str(score_nn)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TEc-VnXRQ32",
    "outputId": "8ebb5ae6-3baa-4920-efef-237b081d6c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 15]\n",
      " [ 1 97]]\n",
      "correctly diagnose nodiabetics is 12/13\n",
      "correctly diagnose diabetics is 97/112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con5 = confusion_matrix(Y_test, Y_pred_nn)\n",
    "print(con5)\n",
    "print(f\"correctly diagnose nodiabetics is {con5[0,0]}/{con5[1,0]+con5[0,0]}\")\n",
    "print(f\"correctly diagnose diabetics is {con5[1,1]}/{con5[0,1]+con5[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xdg5QC7L72-m"
   },
   "source": [
    "# Output final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zOHl3Du75_4",
    "outputId": "3d12fee5-913f-4314-ec1b-887198375c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score achieved using Logistic Regression is: 87.2 %\n",
      "The accuracy score achieved using Support Vector Machine is: 87.2 %\n",
      "The accuracy score achieved using Decision Tree is: 95.2 %\n",
      "The accuracy score achieved using Random Forest is: 92.8 %\n",
      "The accuracy score achieved using Neural Network is: 87.2 %\n"
     ]
    }
   ],
   "source": [
    "scores = [score_lr,score_svm,score_dt,score_rf,score_nn]\n",
    "algorithms = [\"Logistic Regression\",\"Support Vector Machine\",\"Decision Tree\",\"Random Forest\",\"Neural Network\"]    \n",
    "\n",
    "for i in range(len(algorithms)):\n",
    "    print(\"The accuracy score achieved using \"+algorithms[i]+\" is: \"+str(scores[i])+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "qa751RoZ8RD2",
    "outputId": "7b89125a-99cf-4111-9990-b9033f0d22d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8024e821f0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,5)})\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "\n",
    "sns.barplot(algorithms,scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6dXBBNvUQufR",
    "G5wgzehimPmN",
    "OU3d5jTOmPmP",
    "1ub3F0qJmPmQ",
    "3REjx4z3o8Wh",
    "lyqdEoWTvyvM",
    "x-4GmEaGyRxn"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
